{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for running test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_datasets import get_aeon_dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tsml_eval.experiments import experiments, get_regressor_by_name, run_regression_experiment\n",
    "from tsml_eval.evaluation.storage import load_regressor_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_regressor(\n",
    "        regressor, #= TSMLWrapperHydraBoost(),\n",
    "        regressor_name = \"HydraBoost\",\n",
    "    ):\n",
    "    #get HouseholdPowerConsumption1 dataset\n",
    "    current_dir = Path(os.path.dirname(os.getcwd()))\n",
    "    TSER_data_dir = current_dir.parent / \"Data\" / \"TSER\"\n",
    "    dataset_name = \"HouseholdPowerConsumption1\"\n",
    "    X_train, y_train, X_test, y_test = get_aeon_dataset(dataset_name, TSER_data_dir, \"regression\")\n",
    "\n",
    "    #run regression experiment\n",
    "    run_regression_experiment(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        regressor,\n",
    "        regressor_name=regressor_name,\n",
    "        results_path=\"results/\",\n",
    "        dataset_name=dataset_name,\n",
    "        resample_id=0,\n",
    "    )\n",
    "    rr = load_regressor_results(\n",
    "        current_dir / \"exploring-hydra-boosting\" /\"results\" / regressor_name / \"Predictions\" / dataset_name / \"testResample0.csv\"\n",
    "    )\n",
    "    print(rr.predictions)\n",
    "    print(rr.mean_squared_error, \"mse\")\n",
    "    print(rr.root_mean_squared_error, \"rmse\")\n",
    "    print(rr.mean_absolute_percentage_error, \"mape\")\n",
    "    print(rr.r2_score, \"r2\")\n",
    "    print(rr.fit_time, \"fit time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Wrapper no gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin\n",
    "from tsml.base import BaseTimeSeriesEstimator\n",
    "\n",
    "from models.random_feature_representation_boosting import HydraBoost\n",
    "\n",
    "\n",
    "class TSMLWrapperHydraBoost(RegressorMixin, BaseTimeSeriesEstimator):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TSMLWrapperHydraBoost, self).__init__()\n",
    "        self.hydraboost = HydraBoost(\n",
    "            n_layers=1,\n",
    "            init_n_kernels=8,\n",
    "            init_n_groups=64,\n",
    "            n_kernels=8,\n",
    "            n_groups=64,\n",
    "            max_num_channels=3,\n",
    "            hydra_batch_size=10000,\n",
    "            l2_reg=10,\n",
    "            l2_ghat=0.1,\n",
    "            boost_lr=1,\n",
    "            train_top_at = [0, 5, 10],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> object:\n",
    "        \"\"\"Fit the estimator to training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "        y : 1D np.ndarray of shape (n_instances)\n",
    "            The target labels for fitting, indices correspond to instance indices in X\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        y = y.unsqueeze(1)\n",
    "        self.X_mean = X.mean()\n",
    "        self.X_std = X.std()\n",
    "        self.y_mean = y.mean()\n",
    "        self.y_std = y.std()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        y = (y - self.y_mean) / self.y_std\n",
    "        self.hydraboost.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predicts labels for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape (n_instances)\n",
    "            Predicted target labels.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        pred = self.hydraboost(X)\n",
    "        pred = pred * self.y_std + self.y_mean\n",
    "        return pred.squeeze().detach().numpy()\n",
    "        \n",
    "        \n",
    "\n",
    "    def _more_tags(self) -> dict:\n",
    "        return {\n",
    "            \"X_types\": [\"3darray\"],\n",
    "            \"equal_length_only\": True,\n",
    "            \"allow_nan\": False,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training W0\n",
      "Phi0 shape torch.Size([745, 8192])\n",
      "[1600.05651855 1238.1640625  1221.96850586 1557.38879395 1824.94348145\n",
      " 1379.98339844 1697.71594238 1997.15966797 1639.87121582 2560.07348633\n",
      " 1220.82653809 1848.0871582  1699.41491699 1706.4921875  1342.64855957\n",
      " 1542.57836914 1316.07849121 1274.05493164 1155.40625     712.20465088\n",
      "  554.26550293  988.09527588 1601.31396484 2188.98388672 2180.65014648\n",
      " 1841.53637695 1650.64416504 1891.29345703 2146.93457031 2051.04077148\n",
      " 2165.74462891 1893.140625   1877.60656738 1825.11474609 1787.85900879\n",
      " 1318.99133301 2193.06738281 1522.14257812 1709.50732422 1180.23547363\n",
      " 1108.84350586 1007.70184326 1003.35882568  497.15612793 1082.06518555\n",
      " 1270.11193848 2275.28491211 1804.5189209  2094.1105957  1546.29125977\n",
      " 2038.43603516 1813.36242676 1847.92578125 2724.70166016 1399.2956543\n",
      " 2635.95385742 1925.33862305 2269.30444336 1710.20239258 1338.97827148\n",
      " 1648.5020752  1235.19799805 1507.59472656 1129.71191406 1129.39685059\n",
      " 1047.24108887  591.14611816 1255.60461426 1360.73291016 1640.5949707\n",
      " 1930.7701416  1250.21069336 1433.46960449 1659.8581543  1690.43078613\n",
      " 1958.11242676 2129.49536133 1919.29003906 1907.37548828 1604.66967773\n",
      " 1830.79223633 1324.63061523 1447.22753906 2118.45214844 1068.18457031\n",
      " 1821.23950195  614.90075684 1101.42919922 1021.371521    442.4576416\n",
      " 2194.9934082  1694.21618652 2080.1862793  1427.13061523 1904.01586914\n",
      " 1585.83898926 1788.32885742 2300.65454102 1712.08508301 2328.81152344\n",
      " 1640.05334473 2547.28955078 1674.37097168 1545.94384766 1255.58703613\n",
      " 1732.55725098 -420.73010254 1638.42297363  588.34313965 1466.03125\n",
      " 1624.16772461  558.03479004 1593.62963867 1087.40600586 1731.56848145\n",
      " 1424.45751953 1747.3762207  2093.92993164 2555.22802734 1899.37780762\n",
      " 1993.93261719 2395.76220703 1766.17944336 2163.98291016 1690.07580566\n",
      " 1513.46484375 1747.83496094 1559.43249512 1855.71960449 1381.58947754\n",
      " 1092.46899414 1260.0949707   838.612854    467.03820801 1458.15258789\n",
      " 1475.24597168 1614.72399902 2256.42700195 1513.5078125  1460.41662598\n",
      " 2005.16162109 1787.42907715 2065.59448242 1598.17675781 2168.91674805\n",
      " 1956.14831543 1796.72473145 1139.52966309 1761.9432373  1489.90258789\n",
      " 1910.69042969 1241.39892578 1200.5847168   679.1918335  1427.69226074\n",
      " 1527.4239502  1109.34545898 1397.90795898 1042.91455078 2901.47167969\n",
      " 2174.03417969 1469.61083984 1016.35894775 1564.24279785 1359.12792969\n",
      " 1945.52868652 1351.97387695 1874.40454102 1430.63183594 1319.50634766\n",
      " 1492.04003906 1551.54663086 1846.65246582 2124.48144531 1642.49279785\n",
      " 1863.8449707   571.4543457   854.28381348 1212.87438965 1153.25048828\n",
      " 1753.43322754 1331.78271484 2733.00537109 2063.12866211 2516.16186523\n",
      " 1093.7277832  1636.70837402 1626.15405273 1897.31103516  913.29205322\n",
      " 2270.09521484 1954.59289551 2027.16503906 1823.17504883 1546.18530273\n",
      " 1420.46350098 1578.42236328 1034.19360352 1124.38452148  767.49560547\n",
      "  738.01776123  919.49261475  951.00500488 1277.71801758 1606.54870605\n",
      " 2869.76660156 1765.24621582 1519.83947754 2596.0402832  2672.21875\n",
      " 1775.18554688 1770.20898438  844.64825439 1687.45324707 1652.57971191\n",
      " 1559.35168457 1525.17285156 1421.10681152 1137.01721191 1351.96374512\n",
      " 1149.55969238 1389.14819336  875.28356934  583.90753174 1138.11352539\n",
      " 1209.4576416  1329.44165039 1844.56384277 2408.09228516 1465.20092773\n",
      " 1676.49511719 1527.1685791  1684.1192627  2220.91210938  769.49206543\n",
      " 1953.23974609 1346.91271973 1451.43786621 1514.65319824 1697.4050293\n",
      " 1531.0390625  1430.59838867 1096.17822266 1430.96362305 1087.60131836\n",
      "  770.69482422  914.56622314 1991.39147949  993.66668701 2230.6953125\n",
      " 1549.11889648 1289.04516602 1789.37109375 1517.4251709  1345.83178711\n",
      " 2130.58056641 1711.60351562 1909.19055176 1560.88500977  860.0222168\n",
      " 1706.91137695 1636.0637207  1392.62792969 2084.34960938 1132.87890625\n",
      "  971.01599121 1323.96044922 1186.63378906  549.05285645  720.38531494\n",
      " 1241.90905762 1327.17626953 1910.89355469 2334.59814453 1681.40368652\n",
      " 1525.6652832  1818.55175781 2107.74194336 1326.56079102 1766.71240234\n",
      " 1938.6373291  1478.46789551  301.55187988 1436.29943848 1803.69238281\n",
      " 1261.58300781 1043.30419922 1195.11206055 1733.46032715  823.74145508\n",
      "  851.29040527  980.8560791  2004.31604004 1662.64257812 1867.46411133\n",
      " 1976.16088867 1901.95898438 1389.72631836 2375.53564453 1200.53417969\n",
      " 2005.73144531 1363.44116211 2679.12792969 1980.93237305 1128.38342285\n",
      " 1590.62145996 1534.57678223 1557.74597168 1309.32141113  887.27209473\n",
      " 1769.86523438  791.94299316 1090.29675293 1397.80725098 1286.74951172\n",
      " 1055.04785156 1563.67871094 1702.39660645 1443.79882812 1775.95275879\n",
      " 2273.83129883 2262.23144531 1995.80273438 2232.81298828 1517.06970215\n",
      " 1967.89221191 1596.45483398 1518.34997559 1401.92797852 1068.22729492\n",
      " 2023.89160156 1285.94030762 1786.8614502   679.61541748  975.1552124\n",
      " 1631.74450684  953.27026367 1221.55810547 1594.75085449 1829.87255859\n",
      " 2454.52026367 1887.84912109 2315.40966797 1283.35595703 1674.04870605\n",
      " 1639.98083496 2412.83764648 1585.14013672 1314.4942627  1860.69995117\n",
      " 1168.55322266 1233.84887695 1505.4855957  1301.00109863 1198.42419434\n",
      " 1444.13037109 1064.71154785  982.33355713 1158.01794434  761.11651611\n",
      " 1659.88928223 1356.96472168 2240.47460938 2037.6151123  2523.16137695\n",
      " 2421.98730469 2085.36889648 1737.98718262 2836.11816406 1824.09729004\n",
      " 1965.21459961 1535.75280762 1905.32763672 1176.39160156 1506.99243164\n",
      " 1468.22583008 1352.14013672 1679.86791992 1197.73876953  536.01550293\n",
      "  689.61791992 1238.7668457   718.34759521 1517.22729492 1269.82763672\n",
      " 2687.10205078 1915.50805664 2443.15722656 1198.15722656 1937.83544922\n",
      " 1162.35253906 2142.06054688 1302.36706543 1709.19677734 2134.28271484\n",
      " 1550.48034668 1957.18530273 1411.24853516 1386.39465332 1170.83325195\n",
      "  910.79925537 1137.11328125  459.71716309 1063.47924805 1045.9140625\n",
      " 1070.10498047 1319.921875     31.10107422 1659.42651367 1971.2019043\n",
      " 1107.29248047 1252.96875    1622.58227539 2249.6184082  1263.83044434\n",
      " 2023.52600098 1241.49230957 1499.02624512 1937.54309082  934.46081543\n",
      " 1620.83776855 1369.61828613 1313.90246582 1051.59204102  495.88647461\n",
      "  881.23382568 1228.61669922  791.61297607 1555.77197266 1539.58044434\n",
      " 2646.24926758 1234.59484863 1322.26611328 1613.55847168 2106.51904297\n",
      " 2046.18530273 1782.3392334  1665.29394531 1783.28125    1230.63745117\n",
      " 1124.81872559 1711.12768555 1345.7265625  1798.72094727 1233.57678223\n",
      "  647.25439453 1066.85681152  776.15130615 1990.00866699 1561.09619141\n",
      " 2600.43041992 1894.83032227 1355.88684082 1537.46398926 1831.4708252\n",
      " 2115.59326172 1878.17468262  748.90899658 2011.48022461 1978.65612793\n",
      " 1829.24914551  866.11456299 1475.61242676 1471.12524414 1059.60192871\n",
      " 1415.25939941 1159.00585938  883.82891846 1174.97546387 1397.06115723\n",
      " 1300.38439941 1261.34985352 1577.01977539 2012.61657715 1897.00341797\n",
      " 1342.23291016 1701.80310059 1953.45092773 2411.26708984 1636.57495117\n",
      " 1695.04370117 1038.65454102 1197.19104004 1862.04016113 1142.76220703\n",
      " 1404.86682129 1031.39794922  492.42871094 1332.28137207 1589.11987305\n",
      " 1259.51647949 1564.40869141 2116.43554688 2237.81494141 1650.14147949\n",
      " 1897.59399414 2061.18310547 1317.22973633 1580.61584473 1779.28051758\n",
      " 2701.24609375 1811.6114502  1011.36590576 1399.19519043 1794.68359375\n",
      " 1786.59472656 1470.77380371 1379.63220215 1372.06359863  920.91766357\n",
      " 1124.71899414  553.44750977  445.6505127  1487.66186523 1316.41943359\n",
      " 1923.98974609 2677.62597656 1707.7512207  1894.34155273 1725.79797363\n",
      " 2453.45556641 1575.42736816 1817.36010742 1743.11340332 1155.29870605\n",
      " 1688.81640625 1667.23632812 1232.69836426 1102.42004395  786.58831787\n",
      "  549.95556641 1505.3605957  1484.18811035 1899.87854004 1261.87927246\n",
      " 3264.64746094 1707.60888672 1990.6105957  1382.37011719 2165.07055664\n",
      " 1850.22668457 2063.10302734 1157.78747559 1389.00598145  987.39727783\n",
      "  617.37652588 1453.79785156 1338.30444336 1831.35144043 1864.22302246\n",
      " 1619.34521484 1526.68322754 1838.3885498  2598.44824219 1809.88049316\n",
      " 2666.08447266 2064.65820312 2058.81665039  632.58825684 2269.13964844\n",
      " 1685.79797363 1991.76855469 1347.34240723 1038.14916992 1387.04248047\n",
      " 1498.45349121 1336.44360352  559.84753418  590.75854492 1015.39550781\n",
      " 1602.47473145 1522.64172363 2201.0144043  1281.27758789 1652.72558594\n",
      " 1597.36987305 1670.08740234 2253.96899414 1796.71520996 1995.03222656\n",
      " 1497.46923828 1258.74267578 1519.45068359 1244.08764648 1728.1854248\n",
      " 1783.74511719 1265.13122559 2273.65039062 1409.08996582 1012.7399292\n",
      "  701.56427002  535.27062988 1536.9050293  1414.08630371 1616.52233887\n",
      " 2073.37792969 1366.48632812 1349.94506836 1540.01489258 1611.9473877\n",
      " 2369.68261719 1991.83178711 2479.90087891 1912.50109863  735.33258057\n",
      " 1485.59594727 1693.03869629 1499.72973633 1335.76965332 1490.41296387\n",
      " 1925.03955078 1050.35742188  967.27539062  677.53924561  541.56237793\n",
      " 1239.26245117 1559.66699219 2077.14990234 1865.80102539 1726.46618652\n",
      " 1778.59899902 1568.35412598 1675.98083496 1695.77990723 3027.36279297\n",
      " 1758.39257812 1720.82739258 1341.99121094 1618.35998535 1656.79675293\n",
      "  992.91717529 1749.59851074 1359.77624512 1143.10449219 1270.78076172\n",
      "  902.60083008  506.75427246  546.04711914 1302.57495117 1501.11694336\n",
      " 1655.52429199 1924.3861084  1463.0045166  1479.7331543  2435.13891602\n",
      " 1788.41125488 2050.44458008 2515.55249023 2256.27978516 2638.23095703\n",
      " 2635.75830078 1656.40124512 1573.03027344 1779.85107422 1663.45825195\n",
      " 1136.54064941 1391.08544922 1381.07226562 1140.5970459   508.61950684\n",
      "  509.20385742 1072.22705078 1384.7421875  1757.15600586 2632.375\n",
      " 1357.05773926 1396.3815918  1642.22399902 1890.72119141 1758.28881836\n",
      " 1644.63574219 2201.01318359 1876.73840332 1808.60754395 1989.64941406\n",
      " 1320.96887207 1345.09838867 1682.03137207 1397.5925293  1598.30993652\n",
      "  942.83929443 1033.30761719 1048.69165039  495.21520996 1513.63464355\n",
      " 1398.22216797]\n",
      "20861.441652934274 mse\n",
      "144.43490455196167 rmse\n",
      "0.13368404303139342 mape\n",
      "0.9217702074249045 r2\n",
      "39443.0 fit time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nz423\\Code\\exploring-hydra-boosting\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_regressor(\n",
    "    regressor = TSMLWrapperHydraBoost(),\n",
    "    regressor_name = \"HydraBoost\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SKLearnWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, modelClass=None, **model_params,):\n",
    "        self.modelClass = modelClass\n",
    "        self.model_params = model_params\n",
    "        self.seed = None\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        self.modelClass = params.pop('modelClass', self.modelClass)\n",
    "        self.seed = params.pop('seed', self.seed)\n",
    "        self.model_params.update(params)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {'modelClass': self.modelClass}\n",
    "        params.update(self.model_params)\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        self.model = self.modelClass(**self.model_params)\n",
    "        self.model.fit(X, y)\n",
    "        # #classes, either label for binary or one-hot for multiclass\n",
    "        # if len(y.size()) == 1 or y.size(1) == 1:\n",
    "        #     self.classes_ = np.unique(y.detach().cpu().numpy())\n",
    "        # else:\n",
    "        #     self.classes_ = np.unique(y.argmax(axis=1).detach().cpu().numpy())\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model(X).squeeze()#.detach().cpu().squeeze().numpy()\n",
    "        # #binary classification\n",
    "        # if len(self.classes_) == 2:\n",
    "        #     proba_1 = torch.sigmoid(self.model(X))\n",
    "        #     return (proba_1 > 0.5).detach().cpu().numpy()\n",
    "        # else:\n",
    "        #     #multiclass\n",
    "        #     return torch.argmax(self.model(X), dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    # def predict_proba(self, X):\n",
    "    #     #binary classification\n",
    "    #     if len(self.classes_) == 2:\n",
    "    #         proba_1 = torch.nn.functional.sigmoid(self.model(X))\n",
    "    #         return torch.cat((1 - proba_1, proba_1), dim=1).detach().cpu().numpy()\n",
    "    #     else:\n",
    "    #         #multiclass\n",
    "    #         logits = self.model(X)\n",
    "    #         proba = torch.nn.functional.softmax(logits, dim=1)\n",
    "    #         return proba.detach().cpu().numpy()\n",
    "    \n",
    "    # def decision_function(self, X):\n",
    "    #     logits = self.model(X)\n",
    "    #     return logits.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "    # def score(self, X, y):\n",
    "    #     logits = self.model(X)\n",
    "    #     if y.size(1) == 1:\n",
    "    #         y_true = y.detach().cpu().numpy()\n",
    "    #         y_score = logits.detach().cpu().numpy()\n",
    "    #         auc = roc_auc_score(y_true, y_score)\n",
    "    #         return auc\n",
    "    #     else:\n",
    "    #         pred = torch.argmax(logits, dim=1)\n",
    "    #         y = torch.argmax(y, dim=1)\n",
    "    #         acc = (pred == y).float().mean()\n",
    "    #         return acc.detach().cpu().item()\n",
    "    \n",
    "    \n",
    "    \n",
    "class TSMLGridSearchWrapper(RegressorMixin, BaseTimeSeriesEstimator):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 holdour_or_kfold: Literal[\"holdout\", \"kfold\"] = \"kfold\",\n",
    "                 kfolds: Optional[int] = 5,\n",
    "                 holdout_percentage: Optional[float] = 0.2,\n",
    "                 seed: Optional[int] = None,\n",
    "                 modelClass=None, \n",
    "                 model_param_grid: Dict[str, List[Any]] = {}\n",
    "        ):\n",
    "        self.holdour_or_kfold = holdour_or_kfold\n",
    "        self.kfolds = kfolds\n",
    "        self.holdout_percentage = holdout_percentage\n",
    "        self.seed = seed\n",
    "        self.modelClass = modelClass\n",
    "        self.model_param_grid = model_param_grid\n",
    "        super(TSMLGridSearchWrapper, self).__init__()\n",
    "        \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> object:\n",
    "        \"\"\"Fit the estimator to training data, with gridsearch hyperparameter optimization\n",
    "        on holdout or kfold cross-validation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "        y : 1D np.ndarray of shape (n_instances)\n",
    "            The target labels for fitting, indices correspond to instance indices in X\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "        \"\"\"\n",
    "        # TODO regression only\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        y = y.unsqueeze(1)\n",
    "        self.X_mean = X.mean()\n",
    "        self.X_std = X.std()\n",
    "        self.y_mean = y.mean()\n",
    "        self.y_std = y.std()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        y = (y - self.y_mean) / self.y_std\n",
    "        \n",
    "        # Configure cross validation\n",
    "        if self.holdour_or_kfold == \"kfold\":\n",
    "            cv = KFold(n_splits=self.kfolds, shuffle=True, random_state=self.seed)\n",
    "        else:  # holdout\n",
    "            cv = ShuffleSplit(n_splits=1, test_size=self.holdout_percentage, random_state=self.seed)\n",
    "                \n",
    "        # Perform grid search\n",
    "        self.grid_search = GridSearchCV(\n",
    "            estimator=SKLearnWrapper(modelClass=self.modelClass),\n",
    "            param_grid={**self.model_param_grid, \"seed\": [self.seed]},\n",
    "            cv=cv,\n",
    "            scoring=\"neg_mean_squared_error\", # TODO regression only???\n",
    "        )\n",
    "        self.grid_search.fit(X, y)\n",
    "\n",
    "        # Store best model\n",
    "        self.best_model = self.grid_search.best_estimator_\n",
    "        self.best_params = self.grid_search.best_params_\n",
    "        print(\"self.best_params\", self.best_params)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predicts labels for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape (n_instances)\n",
    "            Predicted target labels.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        pred = self.best_model.predict(X) #TODO regression only?\n",
    "        pred = pred * self.y_std + self.y_mean\n",
    "        return pred.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "\n",
    "    def _more_tags(self) -> dict:\n",
    "        return {\n",
    "            \"X_types\": [\"3darray\"],\n",
    "            \"equal_length_only\": True,\n",
    "            \"allow_nan\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training W0\n",
      "Phi0 shape torch.Size([496, 8192])\n",
      "training W0\n",
      "Phi0 shape torch.Size([497, 8192])\n",
      "training W0\n",
      "Phi0 shape torch.Size([497, 8192])\n",
      "training W0\n",
      "Phi0 shape torch.Size([496, 8192])\n",
      "training W0\n",
      "Phi0 shape torch.Size([497, 8192])\n",
      "training W0\n",
      "Phi0 shape torch.Size([497, 8192])\n",
      "training W0\n",
      "Phi0 shape torch.Size([745, 8192])\n",
      "self.best_params {'boost_lr': 1, 'hydra_batch_size': 10000, 'init_n_groups': 64, 'init_n_kernels': 8, 'l2_ghat': 0.01, 'l2_reg': 10, 'max_num_channels': 3, 'n_groups': 64, 'n_kernels': 8, 'n_layers': 1, 'seed': 0, 'train_top_at': [0, 5, 10]}\n",
      "[1642.73950195 1339.05163574 1205.52783203 1630.63696289 1818.23083496\n",
      " 1494.93847656 1640.51171875 1933.3046875  1617.32409668 2562.12304688\n",
      " 1346.82409668 1899.80639648 1716.19604492 1637.89196777 1442.05383301\n",
      " 1544.9864502  1229.63378906 1350.53833008 1082.3449707   690.6918335\n",
      "  638.54089355  955.67822266 1526.09423828 2236.4152832  2277.37353516\n",
      " 1902.18896484 1800.43994141 1742.06860352 2168.48999023 1951.28857422\n",
      " 2112.24658203 1903.06274414 1801.31518555 1874.48242188 1859.01855469\n",
      " 1322.28479004 2173.31518555 1574.18847656 1655.86193848 1082.25024414\n",
      " 1095.12878418 1114.03271484 1047.8548584   597.78051758 1038.98461914\n",
      " 1303.20605469 2253.56323242 1723.49963379 2034.93579102 1612.12060547\n",
      " 2087.44580078 1818.11535645 1772.61157227 2744.85498047 1247.07824707\n",
      " 2560.65917969 1989.07397461 2470.29760742 1557.93493652 1477.46374512\n",
      " 1633.09326172 1145.62976074 1503.90869141 1029.96276855 1180.50341797\n",
      " 1042.78039551  632.3770752  1272.56982422 1399.23864746 1642.28930664\n",
      " 1948.77185059 1171.21679688 1492.73681641 1614.92004395 1736.98547363\n",
      " 1911.76013184 2127.73510742 1882.81799316 1822.17456055 1736.08361816\n",
      " 1879.5847168  1345.36206055 1393.94384766 2022.84460449 1095.91918945\n",
      " 1824.07971191  583.34423828 1155.22558594  989.34307861  511.60681152\n",
      " 2094.38745117 1722.17578125 2046.22375488 1352.78173828 1895.23046875\n",
      " 1660.07312012 1731.82177734 2097.08959961 1781.90527344 2324.16601562\n",
      " 1666.43286133 2452.16259766 1674.61340332 1574.86303711 1285.82897949\n",
      " 1825.09912109 -424.10107422 1735.31481934  556.38464355 1395.2277832\n",
      " 1137.98657227  582.32800293 1520.18273926 1133.17700195 1657.66162109\n",
      " 1140.20837402 1744.16174316 2196.03149414 2528.24902344 1903.29858398\n",
      " 1937.81396484 2400.61206055 1867.83740234 2033.21081543 1606.47619629\n",
      " 1488.1628418  1713.01782227 1518.79248047 1812.65454102 1296.10949707\n",
      " 1037.06591797 1321.91015625  839.3326416   553.40405273 1444.98791504\n",
      " 1361.51306152 1591.29223633 2239.00927734 1456.52331543 1524.89758301\n",
      " 1983.55981445 1803.5078125  2051.37915039 1630.1237793  2078.80688477\n",
      " 1992.50048828 1748.7253418  1144.03967285 1736.56140137 1431.26806641\n",
      " 1881.77844238 1289.37634277 1194.0447998   693.58123779 1535.04553223\n",
      " 1383.28125    1097.21801758 1345.91967773 1028.05749512 2837.\n",
      " 2104.56201172 1480.5690918  1015.63549805 1577.92236328 1457.95727539\n",
      " 1832.82177734 1450.91516113 1757.61621094 1372.79504395 1316.9798584\n",
      " 1475.02575684 1595.76464844 1874.80822754 2089.00390625 1613.16235352\n",
      " 1737.87695312  527.28942871  882.82659912 1221.37145996 1158.42822266\n",
      " 1695.12084961 1341.4564209  2857.48217773 2023.17529297 2638.1015625\n",
      " 1094.5690918  1558.05749512 1580.59069824 1798.19714355  910.10656738\n",
      " 2090.40234375 1951.00878906 1991.21887207 1767.63574219 1557.44177246\n",
      " 1421.87365723 1584.6340332  1040.12890625 1134.08911133  774.95727539\n",
      "  746.49371338  878.29187012 1076.17724609 1177.58569336 1720.89135742\n",
      " 3008.44921875 1813.42932129 1457.85632324 2619.14794922 2633.90136719\n",
      " 1764.5534668  1724.04248047  962.14996338 1795.55908203 1747.90856934\n",
      " 1569.99243164 1528.8848877  1492.9284668  1237.83276367 1356.63977051\n",
      " 1139.32568359 1288.49084473  896.82995605  713.3449707  1097.30065918\n",
      " 1226.92529297 1362.54772949 1810.71179199 2284.63598633 1394.79406738\n",
      " 1749.89782715 1386.7154541  1592.38366699 2224.00585938  838.41638184\n",
      " 1971.13049316 1222.69396973 1518.61816406 1379.11352539 1795.51171875\n",
      " 1634.14758301 1499.92382812 1062.0390625  1383.28381348 1075.44775391\n",
      "  695.99224854  950.71411133 2038.3972168   958.17834473 2311.33251953\n",
      " 1335.13256836 1233.07885742 1952.83959961 1530.58874512 1364.06933594\n",
      " 2204.49804688 1725.51916504 1943.19213867 1581.86401367  929.30474854\n",
      " 1670.63269043 1633.22937012 1421.45458984 2111.56494141 1124.27978516\n",
      " 1019.57635498 1260.28601074 1262.63647461  599.47216797  686.35333252\n",
      " 1221.55957031 1313.21618652 1871.94824219 2315.54345703 1648.67248535\n",
      " 1601.18371582 1864.578125   2173.07983398 1272.95996094 1842.40478516\n",
      " 1967.90795898 1502.14941406  581.55169678 1447.26049805 1754.73925781\n",
      " 1333.6550293  1113.79516602 1130.38208008 1662.9888916   718.49041748\n",
      "  871.79626465  940.77374268 1999.05126953 1793.28308105 1877.09680176\n",
      " 1886.51171875 1923.47290039 1510.97717285 2369.16381836 1142.29077148\n",
      " 1997.49609375 1432.79931641 2747.48632812 2002.01660156 1335.90393066\n",
      " 1653.19165039 1469.38549805 1521.61657715 1307.79882812  893.84295654\n",
      " 1784.12890625  769.68170166 1080.05249023 1326.55395508 1379.86779785\n",
      " 1051.28234863 1538.29577637 1720.55126953 1342.4519043  1682.35766602\n",
      " 2382.88623047 2287.80175781 1939.95947266 2222.37402344 1475.18041992\n",
      " 1847.51879883 1666.71191406 1431.88586426 1525.98742676 1096.2791748\n",
      " 2070.16845703 1213.95800781 1659.15917969  708.54376221 1013.1809082\n",
      " 1536.33203125  493.61523438 1241.38232422 1486.04980469 1915.52600098\n",
      " 2527.54394531 1836.64074707 2370.06567383 1300.5213623  1706.97814941\n",
      " 1703.8079834  2422.94555664 1589.56335449 1425.97277832 1786.81774902\n",
      " 1189.14599609 1228.54260254 1484.84545898 1311.58886719 1187.11303711\n",
      " 1489.56555176 1074.12939453 1000.21704102 1095.44555664  782.49041748\n",
      " 1653.15002441 1405.97631836 2169.36914062 1989.81018066 2540.91015625\n",
      " 2485.94775391 1986.06420898 1873.35974121 2723.19384766 1709.11328125\n",
      " 1876.18969727 1683.11657715 1861.87487793 1148.51062012 1501.78491211\n",
      " 1457.64575195 1372.58325195 1673.27172852 1285.31958008  601.35198975\n",
      "  647.45361328 1311.19934082  731.35217285 1450.26611328 1282.9309082\n",
      " 2658.47460938 1835.47851562 2404.6862793  1233.21166992 1952.54943848\n",
      " 1267.47241211 2155.69189453 1238.10583496 1653.57275391 2119.56152344\n",
      " 1472.00061035 1941.3828125  1396.76806641 1524.96435547 1228.24987793\n",
      "  884.80755615 1127.14880371  459.45141602  996.45043945 1024.29882812\n",
      " 1108.1184082  1294.56054688  316.12487793 1591.32849121 1893.05041504\n",
      " 1141.05883789 1293.64453125 1662.48059082 2227.14575195 1246.31933594\n",
      " 2181.44580078 1237.5456543  1512.53381348 1840.27124023 1023.20343018\n",
      " 1535.91931152 1513.78027344 1389.64709473 1046.13745117  525.22314453\n",
      "  993.58172607 1192.86962891  825.51495361 1546.34130859 1519.7232666\n",
      " 2501.17626953 1198.25231934 1409.92980957 1456.69641113 1964.24658203\n",
      " 2085.00415039 1800.97033691 1741.10656738 1738.19824219 1243.85070801\n",
      " 1130.55969238 1548.1081543  1377.7019043  1810.60705566 1372.03320312\n",
      "  581.26428223 1121.93115234  742.30670166 2049.45581055 1594.31311035\n",
      " 2544.63623047 1818.08325195 1366.45507812 1655.57055664 1776.72729492\n",
      " 2069.25244141 1800.15380859  698.96728516 2121.47827148 1963.35546875\n",
      " 1806.4765625   844.36291504 1439.37988281 1489.4901123  1124.4197998\n",
      " 1391.13818359 1061.59057617  935.04656982 1132.94958496 1425.96008301\n",
      " 1308.38964844 1216.23657227 1559.19445801 1991.61621094 1885.73242188\n",
      " 1323.70288086 1770.90917969 1940.04541016 2480.20898438 1573.8293457\n",
      " 1674.5715332  1067.03222656 1291.5880127  1894.30651855 1135.64624023\n",
      " 1424.96435547  979.13909912  601.75286865 1298.5057373  1613.98754883\n",
      " 1282.13110352 1663.76513672 2068.17675781 2201.30859375 1746.51184082\n",
      " 1922.50817871 2043.24633789 1422.09033203 1491.65039062 1879.37158203\n",
      " 2739.68041992 1898.21386719 1105.95410156 1448.65087891 1766.86743164\n",
      " 1739.28625488 1526.73583984 1298.62719727 1437.05114746  944.30743408\n",
      " 1080.21289062  481.20080566  505.74084473 1445.62658691 1357.2824707\n",
      " 1860.06335449 2641.46044922 1703.36437988 1904.61401367 1733.62133789\n",
      " 2481.58935547 1620.9708252  1808.64526367 1718.3885498  1140.08764648\n",
      " 1634.84729004 1635.20471191 1224.49243164 1141.62536621  810.57189941\n",
      "  638.61621094 1487.5291748  1451.99487305 1862.21276855 1280.64648438\n",
      " 3207.82006836 1812.94140625 1991.87854004 1346.72387695 2063.43139648\n",
      " 1909.35180664 1974.85766602 1101.41699219 1399.85522461 1031.27124023\n",
      "  592.14678955 1376.26940918 1425.28088379 1871.92687988 1799.84069824\n",
      " 1718.43054199 1565.21704102 1830.09570312 2735.99316406 1709.25549316\n",
      " 2624.51220703 2135.99243164 1953.56469727  633.23950195 2288.01708984\n",
      " 1669.65844727 1916.29309082 1315.49938965  939.2409668  1382.26623535\n",
      " 1375.95056152 1289.97351074  542.32055664  636.70788574  916.38391113\n",
      " 1633.578125   1595.9395752  2121.21386719 1323.67712402 1663.82861328\n",
      " 1610.87609863 1720.35925293 2209.24609375 1801.38818359 2036.31958008\n",
      " 1541.39050293 1342.48803711 1568.25830078 1321.57617188 1768.35510254\n",
      " 1778.63818359 1272.41308594 2338.24438477 1414.29553223 1044.55371094\n",
      "  682.4017334   571.57849121 1536.16479492 1387.68884277 1687.84753418\n",
      " 2001.94287109 1339.08557129 1290.89135742 1577.09326172 1692.96166992\n",
      " 2278.28442383 1999.75170898 2449.65722656 1866.1751709   903.16088867\n",
      " 1498.8840332  1662.28271484 1461.91699219 1372.96875    1542.70812988\n",
      " 1889.34277344 1080.35925293 1009.05090332  522.70275879  589.82263184\n",
      " 1190.68127441 1556.2331543  2061.06152344 1836.15454102 1694.78881836\n",
      " 1799.34057617 1583.60766602 1711.24719238 1740.80773926 3215.06738281\n",
      " 1759.65307617 1738.859375   1453.01342773 1597.30236816 1658.82055664\n",
      " 1037.38964844 1651.93041992 1302.26953125 1092.13818359 1304.0703125\n",
      "  912.85241699  517.01599121  618.39916992 1282.30981445 1648.15869141\n",
      " 1656.53637695 1758.64257812 1480.97497559 1473.74536133 2393.07543945\n",
      " 1801.95678711 1960.59936523 2515.89697266 2241.02001953 2645.86499023\n",
      " 2755.58007812 1689.12878418 1551.04882812 1862.13269043 1646.06054688\n",
      " 1172.38049316 1291.66394043 1393.41540527 1132.92883301  373.0057373\n",
      "  555.67089844 1097.50976562 1433.19140625 1747.30151367 2641.86181641\n",
      " 1332.90869141 1297.67675781 1585.03625488 1979.40612793 1810.15124512\n",
      " 1718.02734375 2149.12402344 1927.5880127  1817.77856445 2035.78320312\n",
      " 1236.984375   1238.86584473 1745.08227539 1423.0090332  1542.9732666\n",
      "  931.99316406  989.55053711 1004.00909424  556.94677734 1499.94848633\n",
      " 1439.41235352]\n",
      "17506.24797523186 mse\n",
      "132.3111785724542 rmse\n",
      "0.12988868528296538 mape\n",
      "0.9343520850258235 r2\n",
      "173246.0 fit time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nz423\\Code\\exploring-hydra-boosting\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_regressor(\n",
    "    TSMLGridSearchWrapper(\n",
    "        \"kfold\",\n",
    "        kfolds=3,\n",
    "        seed=0,\n",
    "        modelClass=HydraBoost,\n",
    "        model_param_grid={\n",
    "            \"n_layers\": [1],              # [0,1,3,6,10] ?\n",
    "            \"init_n_kernels\": [8],\n",
    "            \"init_n_groups\": [64],\n",
    "            \"n_kernels\": [8],\n",
    "            \"n_groups\": [64],\n",
    "            \"max_num_channels\": [3],\n",
    "            \"hydra_batch_size\": [10000],\n",
    "            \"l2_reg\": [10],                # [0.0001, 0.001, 0.01, 0.1, 1] ?\n",
    "            \"l2_ghat\": [0.01, 1],          # [0.0001, 0.001, 0.01, 0.1, 1] ?\n",
    "            \"boost_lr\": [1],\n",
    "            \"train_top_at\": [[0, 5, 10]],\n",
    "        },\n",
    "    ),\n",
    "regressor_name = \"HydraBoostGridSearch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSMLOptunaWrapper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
