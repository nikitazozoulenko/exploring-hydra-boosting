{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster/local configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nz423\\Code\\exploring-hydra-boosting\n",
      "c:\\Users\\nz423\\Data\\TSER\n",
      "c:\\Users\\nz423\\Data\\BigTSC\n",
      "c:\\Users\\nz423\\Data\\UTSC\n"
     ]
    }
   ],
   "source": [
    "# If on cluster, cwd is '/rds/general/user/nz423', not the project directory, breaking imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_dir = Path(os.getcwd()) / \"exploring-hydra-boosting\"\n",
    "if \"rds\" not in project_dir.parts:\n",
    "    project_dir = project_dir.parent\n",
    "    datasets_dir = project_dir.parent.parent / \"Data\"\n",
    "else:\n",
    "    datasets_dir = project_dir / \"DATASETS\"\n",
    "    if str(project_dir) not in sys.path:\n",
    "        sys.path.append(str(project_dir))\n",
    "    \n",
    "class Config:\n",
    "    project_dir = project_dir\n",
    "    TSER_dir = datasets_dir / \"TSER\"\n",
    "    BigTSC_dir = datasets_dir / \"BigTSC\"\n",
    "    UTSC_dir = datasets_dir / \"UTSC\"\n",
    "\n",
    "print(Config.project_dir)\n",
    "print(Config.TSER_dir)\n",
    "print(Config.BigTSC_dir)\n",
    "print(Config.UTSC_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsml_eval.experiments import experiments, get_regressor_by_name, run_regression_experiment\n",
    "from tsml_eval.evaluation.storage import load_regressor_results\n",
    "\n",
    "from load_datasets import get_aeon_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for running test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_regressor(\n",
    "        regressor, #= TSMLWrapperHydraBoost(),\n",
    "        regressor_name, # = \"HydraBoost\",\n",
    "        TSER_data_dir: Path = Config.TSER_dir,\n",
    "        project_dir: Path = Config.project_dir,\n",
    "    ):\n",
    "    #get HouseholdPowerConsumption1 dataset\n",
    "    dataset_name = \"HouseholdPowerConsumption1\"\n",
    "    X_train, y_train, X_test, y_test = get_aeon_dataset(dataset_name, TSER_data_dir, \"regression\")\n",
    "\n",
    "    #run regression experiment\n",
    "    run_regression_experiment(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        regressor,\n",
    "        regressor_name = regressor_name,\n",
    "        results_path = project_dir / \"results\",\n",
    "        dataset_name = dataset_name,\n",
    "        resample_id=0,\n",
    "    )\n",
    "    rr = load_regressor_results(\n",
    "        project_dir / \"results\" / regressor_name / \"Predictions\" / dataset_name / \"testResample0.csv\"\n",
    "    )\n",
    "    print(rr.predictions)\n",
    "    print(rr.mean_squared_error, \"mse\")\n",
    "    print(rr.root_mean_squared_error, \"rmse\")\n",
    "    print(rr.mean_absolute_percentage_error, \"mape\")\n",
    "    print(rr.r2_score, \"r2\")\n",
    "    print(rr.fit_time, \"fit time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Wrapper no gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin\n",
    "from tsml.base import BaseTimeSeriesEstimator\n",
    "\n",
    "from models.random_feature_representation_boosting import HydraBoost\n",
    "\n",
    "\n",
    "class TSMLWrapperHydraBoost(RegressorMixin, BaseTimeSeriesEstimator):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TSMLWrapperHydraBoost, self).__init__()\n",
    "        self.hydraboost = HydraBoost(\n",
    "            n_layers=1,\n",
    "            init_n_kernels=8,\n",
    "            init_n_groups=64,\n",
    "            n_kernels=8,\n",
    "            n_groups=64,\n",
    "            max_num_channels=3,\n",
    "            hydra_batch_size=10000,\n",
    "            l2_reg=10,\n",
    "            l2_ghat=0.1,\n",
    "            boost_lr=1,\n",
    "            train_top_at = [0, 5, 10],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> object:\n",
    "        \"\"\"Fit the estimator to training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "        y : 1D np.ndarray of shape (n_instances)\n",
    "            The target labels for fitting, indices correspond to instance indices in X\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        y = y.unsqueeze(1)\n",
    "        self.X_mean = X.mean()\n",
    "        self.X_std = X.std()\n",
    "        self.y_mean = y.mean()\n",
    "        self.y_std = y.std()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        y = (y - self.y_mean) / self.y_std\n",
    "        self.hydraboost.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predicts labels for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape (n_instances)\n",
    "            Predicted target labels.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        pred = self.hydraboost(X)\n",
    "        pred = pred * self.y_std + self.y_mean\n",
    "        return pred.squeeze().detach().numpy()\n",
    "        \n",
    "        \n",
    "\n",
    "    def _more_tags(self) -> dict:\n",
    "        return {\n",
    "            \"X_types\": [\"3darray\"],\n",
    "            \"equal_length_only\": True,\n",
    "            \"allow_nan\": False,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_regressor(\n",
    "#     regressor = TSMLWrapperHydraBoost(),\n",
    "#     regressor_name = \"HydraBoost\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SKLearnWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, modelClass=None, **model_params,):\n",
    "        self.modelClass = modelClass\n",
    "        self.model_params = model_params\n",
    "        self.seed = None\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        self.modelClass = params.pop('modelClass', self.modelClass)\n",
    "        self.seed = params.pop('seed', self.seed)\n",
    "        self.model_params.update(params)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {'modelClass': self.modelClass}\n",
    "        params.update(self.model_params)\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        self.model = self.modelClass(**self.model_params)\n",
    "        self.model.fit(X, y)\n",
    "        # #classes, either label for binary or one-hot for multiclass\n",
    "        # if len(y.size()) == 1 or y.size(1) == 1:\n",
    "        #     self.classes_ = np.unique(y.detach().cpu().numpy())\n",
    "        # else:\n",
    "        #     self.classes_ = np.unique(y.argmax(axis=1).detach().cpu().numpy())\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model(X).squeeze()#.detach().cpu().squeeze().numpy()\n",
    "        # #binary classification\n",
    "        # if len(self.classes_) == 2:\n",
    "        #     proba_1 = torch.sigmoid(self.model(X))\n",
    "        #     return (proba_1 > 0.5).detach().cpu().numpy()\n",
    "        # else:\n",
    "        #     #multiclass\n",
    "        #     return torch.argmax(self.model(X), dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    # def predict_proba(self, X):\n",
    "    #     #binary classification\n",
    "    #     if len(self.classes_) == 2:\n",
    "    #         proba_1 = torch.nn.functional.sigmoid(self.model(X))\n",
    "    #         return torch.cat((1 - proba_1, proba_1), dim=1).detach().cpu().numpy()\n",
    "    #     else:\n",
    "    #         #multiclass\n",
    "    #         logits = self.model(X)\n",
    "    #         proba = torch.nn.functional.softmax(logits, dim=1)\n",
    "    #         return proba.detach().cpu().numpy()\n",
    "    \n",
    "    # def decision_function(self, X):\n",
    "    #     logits = self.model(X)\n",
    "    #     return logits.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "    # def score(self, X, y):\n",
    "    #     logits = self.model(X)\n",
    "    #     if y.size(1) == 1:\n",
    "    #         y_true = y.detach().cpu().numpy()\n",
    "    #         y_score = logits.detach().cpu().numpy()\n",
    "    #         auc = roc_auc_score(y_true, y_score)\n",
    "    #         return auc\n",
    "    #     else:\n",
    "    #         pred = torch.argmax(logits, dim=1)\n",
    "    #         y = torch.argmax(y, dim=1)\n",
    "    #         acc = (pred == y).float().mean()\n",
    "    #         return acc.detach().cpu().item()\n",
    "    \n",
    "    \n",
    "    \n",
    "class TSMLGridSearchWrapper(RegressorMixin, BaseTimeSeriesEstimator):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 holdour_or_kfold: Literal[\"holdout\", \"kfold\"] = \"kfold\",\n",
    "                 kfolds: Optional[int] = 5,\n",
    "                 holdout_percentage: Optional[float] = 0.2,\n",
    "                 seed: Optional[int] = None,\n",
    "                 modelClass=None, \n",
    "                 model_param_grid: Dict[str, List[Any]] = {}\n",
    "        ):\n",
    "        self.holdour_or_kfold = holdour_or_kfold\n",
    "        self.kfolds = kfolds\n",
    "        self.holdout_percentage = holdout_percentage\n",
    "        self.seed = seed\n",
    "        self.modelClass = modelClass\n",
    "        self.model_param_grid = model_param_grid\n",
    "        super(TSMLGridSearchWrapper, self).__init__()\n",
    "        \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> object:\n",
    "        \"\"\"Fit the estimator to training data, with gridsearch hyperparameter optimization\n",
    "        on holdout or kfold cross-validation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "        y : 1D np.ndarray of shape (n_instances)\n",
    "            The target labels for fitting, indices correspond to instance indices in X\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "        \"\"\"\n",
    "        # TODO regression only\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        y = y.unsqueeze(1)\n",
    "        self.X_mean = X.mean()\n",
    "        self.X_std = X.std()\n",
    "        self.y_mean = y.mean()\n",
    "        self.y_std = y.std()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        y = (y - self.y_mean) / self.y_std\n",
    "        \n",
    "        # Configure cross validation\n",
    "        if self.holdour_or_kfold == \"kfold\":\n",
    "            cv = KFold(n_splits=self.kfolds, shuffle=True, random_state=self.seed)\n",
    "        else:  # holdout\n",
    "            cv = ShuffleSplit(n_splits=1, test_size=self.holdout_percentage, random_state=self.seed)\n",
    "                \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=SKLearnWrapper(modelClass=self.modelClass),\n",
    "            param_grid={**self.model_param_grid, \"seed\": [self.seed]},\n",
    "            cv=cv,\n",
    "            scoring=\"neg_mean_squared_error\", # TODO regression only???\n",
    "        )\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # Store best model\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "        print(\"self.best_params\", self.best_params)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predicts labels for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape (n_instances)\n",
    "            Predicted target labels.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        pred = self.best_model.predict(X) #TODO regression only?\n",
    "        pred = pred * self.y_std + self.y_mean\n",
    "        return pred.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "\n",
    "    def _more_tags(self) -> dict:\n",
    "        return {\n",
    "            \"X_types\": [\"3darray\"],\n",
    "            \"equal_length_only\": True,\n",
    "            \"allow_nan\": False,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def get_params(self):\n",
    "        \"\"\"Use for saving model configuration in tsml\"\"\"\n",
    "        if hasattr(self, 'best_params'):\n",
    "            return {\n",
    "            \"seed\": self.seed,\n",
    "            **self.best_params\n",
    "            }\n",
    "        else:\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1632.74804688 1307.1315918  1219.61230469 1678.48181152 1794.53308105\n",
      " 1451.53271484 1685.95910645 1828.33410645 1635.66442871 2580.16064453\n",
      " 1241.31286621 1988.49865723 1749.5534668  1600.86193848 1340.72998047\n",
      " 1606.75219727 1262.25732422 1305.54907227 1197.86352539  744.72027588\n",
      "  610.05841064  988.00250244 1539.81054688 2086.47241211 2328.82128906\n",
      " 1868.50073242 1688.40490723 1745.765625   2035.89953613 2025.31518555\n",
      " 2101.10742188 1903.29968262 1856.94238281 1802.69299316 1893.04846191\n",
      " 1279.20275879 2159.07080078 1610.74255371 1643.16674805 1156.27062988\n",
      " 1103.0300293  1062.07873535 1030.41723633  606.99560547 1114.81530762\n",
      " 1283.39831543 2130.77514648 1840.85681152 2027.20288086 1622.62451172\n",
      " 2068.74487305 1735.21569824 1715.53967285 2685.93310547 1332.39038086\n",
      " 2567.73486328 1949.50585938 2473.5378418  1611.88684082 1470.88623047\n",
      " 1591.21606445 1267.67382812 1560.87463379 1094.10266113 1175.95703125\n",
      " 1087.65930176  574.80493164 1211.80090332 1405.18859863 1620.14624023\n",
      " 2056.35693359 1183.56616211 1495.48205566 1567.15881348 1677.19396973\n",
      " 1699.05407715 2219.36425781 1976.92260742 1844.03662109 1729.00720215\n",
      " 1781.22949219 1341.72460938 1488.37988281 2047.1932373  1046.69433594\n",
      " 1898.09423828  617.90161133 1062.40441895  929.63720703  555.25708008\n",
      " 2169.35400391 1755.49316406 2136.94628906 1388.81396484 1789.44299316\n",
      " 1654.79968262 1690.37060547 2231.9934082  1767.59594727 2296.78955078\n",
      " 1625.07861328 2562.40234375 1685.83642578 1590.77392578 1278.30883789\n",
      " 1699.36706543  -29.93444824 1613.35632324  563.69018555 1316.65490723\n",
      " 1288.73901367  577.23583984 1570.84875488 1107.01977539 1586.47094727\n",
      "  922.3807373  1742.02001953 2175.14770508 2646.36132812 1811.98596191\n",
      " 1898.34912109 2588.67236328 1906.99609375 2216.3190918  1556.05273438\n",
      " 1499.12597656 1694.51525879 1512.92480469 1801.45825195 1364.84558105\n",
      " 1001.99829102 1244.1003418   835.8739624   505.2935791  1456.03100586\n",
      " 1377.99255371 1522.65209961 2295.23046875 1450.79492188 1491.21252441\n",
      " 1817.06176758 1889.21166992 2172.96875    1660.28369141 2131.05810547\n",
      " 1904.38623047 1726.94958496 1099.52624512 1814.31359863 1350.48999023\n",
      " 1996.95068359 1385.87780762 1107.11047363  703.53204346 1550.94152832\n",
      " 1449.90356445 1140.18334961 1415.36767578 1076.63354492 2844.62597656\n",
      " 2154.5949707  1468.15148926 1067.05957031 1619.52331543 1378.12158203\n",
      " 1859.7277832  1389.21081543 1814.1003418  1471.09082031 1315.04370117\n",
      " 1455.82006836 1634.21850586 1784.78613281 2077.44287109 1637.60534668\n",
      " 1785.38684082  520.55969238  832.63397217 1271.00695801 1132.96508789\n",
      " 1665.0690918  1415.00964355 2938.63818359 2067.99682617 2662.29296875\n",
      "  979.26239014 1575.8001709  1591.49414062 1857.62915039  985.79229736\n",
      " 2006.36706543 1909.81201172 1945.75683594 1836.11450195 1655.82836914\n",
      " 1412.79797363 1675.90478516 1057.71362305 1044.19836426  756.08398438\n",
      "  852.84820557  926.64727783 1098.68444824 1282.12182617 1671.15393066\n",
      " 2848.56103516 1924.2824707  1495.45275879 2593.45776367 2607.46777344\n",
      " 1839.14770508 1841.21801758  902.13909912 1817.57568359 1677.24047852\n",
      " 1470.31604004 1557.23254395 1436.60217285 1247.7791748  1442.30200195\n",
      " 1105.28308105 1237.74584961  899.81335449  726.95843506 1160.4699707\n",
      " 1286.94641113 1452.5814209  1735.82800293 2488.19335938 1422.13195801\n",
      " 1764.31213379 1453.62463379 1730.52233887 2229.86499023  813.66925049\n",
      " 1950.51379395 1403.90893555 1461.0065918  1358.7208252  1755.95715332\n",
      " 1571.83581543 1455.76782227 1061.1159668  1447.52380371 1050.20910645\n",
      "  690.70513916  930.17962646 2056.89599609  979.78039551 2243.73364258\n",
      " 1422.01721191 1245.37890625 1852.26489258 1463.05615234 1393.61950684\n",
      " 2199.88671875 1762.43762207 1929.81982422 1516.3671875   827.44622803\n",
      " 1709.98608398 1674.55444336 1371.74902344 2140.24975586 1086.07250977\n",
      "  982.09869385 1219.67163086 1285.90771484  587.15838623  682.50939941\n",
      " 1252.47265625 1327.08105469 1964.15759277 2321.30126953 1650.29675293\n",
      " 1596.72387695 1811.01208496 2229.97827148 1238.11523438 1729.28271484\n",
      " 1912.20996094 1433.39709473  576.56671143 1518.19750977 1783.29528809\n",
      " 1364.99584961 1114.41638184 1202.4375     1591.14074707  696.91186523\n",
      "  817.23376465 1008.8001709  2037.94311523 1718.86535645 1885.60864258\n",
      " 1885.16748047 1879.12744141 1406.01660156 2326.98388672 1213.04187012\n",
      " 2008.79125977 1322.50268555 2757.08007812 1934.79748535 1145.41015625\n",
      " 1662.95166016 1558.90075684 1460.07189941 1341.91723633  896.37561035\n",
      " 1767.23144531  781.42523193 1076.05786133 1377.20141602 1359.68469238\n",
      " 1011.13702393 1521.97924805 1666.14367676 1488.49023438 1715.93933105\n",
      " 2330.15478516 2142.96484375 1987.58959961 2164.01757812 1449.9831543\n",
      " 1698.88085938 1682.42419434 1471.20581055 1477.13378906 1108.85058594\n",
      " 2166.3125     1202.06518555 1620.25073242  724.85601807  918.19012451\n",
      " 1551.04187012  748.33892822 1232.09082031 1514.61804199 1928.11706543\n",
      " 2418.52758789 1723.87524414 2382.45166016 1210.60449219 1614.26599121\n",
      " 1642.21484375 2511.2043457  1499.09643555 1403.91894531 1819.60644531\n",
      " 1114.26196289 1243.93115234 1498.39538574 1409.23864746 1176.22277832\n",
      " 1375.05737305 1128.7166748  1062.94287109 1135.69836426  818.05993652\n",
      " 1633.22741699 1445.27966309 2219.9699707  2028.36340332 2705.23925781\n",
      " 2279.34301758 1915.35717773 1875.74487305 2851.47021484 1732.76599121\n",
      " 1900.40478516 1595.50854492 1836.73669434 1126.62268066 1614.83032227\n",
      " 1389.51135254 1363.64709473 1611.43273926 1285.69604492  532.46447754\n",
      "  612.27264404 1345.59997559  758.66162109 1530.44897461 1281.15942383\n",
      " 2533.84448242 1891.33520508 2344.68701172 1087.90881348 1945.99072266\n",
      " 1244.99121094 2114.29394531 1233.36657715 1679.03259277 2048.5534668\n",
      " 1598.97595215 2020.42687988 1430.31005859 1465.56958008 1240.25634766\n",
      "  856.65100098 1108.7199707   389.48120117  916.05743408  977.52880859\n",
      " 1036.29956055 1290.97167969  211.83056641 1574.53259277 1861.39880371\n",
      " 1144.95178223 1275.58154297 1637.49267578 2204.22802734 1202.71569824\n",
      " 2140.86865234 1225.07507324 1496.41564941 1833.03820801  961.94274902\n",
      " 1562.890625   1285.39624023 1410.07324219 1068.71240234  470.74938965\n",
      "  836.22967529 1209.58203125  860.98065186 1567.04821777 1478.76525879\n",
      " 2483.58984375 1260.64477539 1375.61889648 1469.15002441 2023.55126953\n",
      " 2136.54296875 1846.2512207  1665.39489746 1675.00305176 1268.07629395\n",
      " 1145.28515625 1617.30688477 1364.12963867 1770.7532959  1364.62597656\n",
      "  646.32635498 1102.26806641  714.19213867 2003.63049316 1472.26977539\n",
      " 2503.12207031 1780.19042969 1219.63146973 1567.87670898 1799.4666748\n",
      " 2163.00610352 1864.33752441  675.27197266 2178.25048828 1941.57531738\n",
      " 1805.13513184  855.0645752  1358.58154297 1509.09472656 1006.46838379\n",
      " 1440.71496582 1047.30737305  946.38787842 1205.57714844 1367.25598145\n",
      " 1285.92834473 1641.10583496 1583.42236328 1972.89025879 1789.51928711\n",
      " 1259.39135742 1795.22949219 1929.4263916  2392.33007812 1574.08300781\n",
      " 1724.04125977 1148.38623047 1256.43310547 2028.41381836 1131.96411133\n",
      " 1411.09594727 1045.31103516  565.71911621 1293.58898926 1514.04272461\n",
      " 1242.30664062 1638.84411621 1977.92321777 2232.66601562 1679.46679688\n",
      " 1937.49597168 1957.69519043 1326.9810791  1595.30578613 1918.11218262\n",
      " 2671.41259766 1840.37866211 1066.61914062 1378.10693359 1794.36010742\n",
      " 1653.41003418 1581.93786621 1297.71533203 1442.16174316  983.03375244\n",
      " 1238.31652832  505.52722168  441.06262207 1548.43139648 1342.04797363\n",
      " 1970.10742188 2632.09521484 1656.15734863 2012.87255859 1733.18041992\n",
      " 2593.19482422 1610.0859375  1748.16223145 1824.00866699 1096.38720703\n",
      " 1641.15661621 1657.09020996 1224.18115234 1078.15551758  757.95843506\n",
      "  607.97344971 1495.25292969 1457.55053711 1798.73217773 1367.49597168\n",
      " 3250.63061523 1632.76940918 2047.67553711 1321.47790527 2162.66870117\n",
      " 1963.33178711 1910.42016602 1144.9473877  1488.64477539 1055.48730469\n",
      "  602.02209473 1419.36254883 1327.58239746 1703.29382324 1858.03417969\n",
      " 1572.10778809 1602.05969238 1809.83068848 2560.65722656 1729.2902832\n",
      " 2682.48193359 2198.8449707  2022.32727051  504.77380371 2331.37768555\n",
      " 1596.74731445 2031.82104492 1291.92480469  952.42333984 1617.68823242\n",
      " 1478.34326172 1372.58410645  468.80871582  558.93652344 1071.77172852\n",
      " 1507.67712402 1577.3527832  2239.16845703 1332.83361816 1664.57568359\n",
      " 1640.17956543 1698.4263916  2171.9375     1823.0390625  1998.19946289\n",
      " 1481.7154541  1250.70898438 1561.36865234 1265.19812012 1701.69580078\n",
      " 1747.20166016 1245.84985352 2303.96142578 1376.53808594 1098.54174805\n",
      "  680.90264893  643.51428223 1571.65808105 1328.24060059 1741.55725098\n",
      " 2040.66918945 1411.94445801 1363.75195312 1501.2689209  1665.59692383\n",
      " 2317.07910156 1951.41918945 2462.41821289 1900.36889648  670.96362305\n",
      " 1577.8157959  1756.70715332 1520.2310791  1281.93334961 1494.37390137\n",
      " 1900.7421875  1053.1706543  1045.81445312  686.5826416   547.3326416\n",
      " 1210.12768555 1580.44873047 2015.57666016 1908.57800293 1703.77880859\n",
      " 1837.01916504 1531.33166504 1619.32275391 1757.99145508 3334.99951172\n",
      " 1759.88269043 1759.72741699 1395.31640625 1620.74560547 1692.71716309\n",
      "  972.41766357 1732.79626465 1240.34362793 1087.19970703 1348.14257812\n",
      "  941.26629639  542.50976562  624.39453125 1236.12695312 1531.39282227\n",
      " 1566.82299805 1753.39685059 1513.34729004 1507.23608398 2401.62963867\n",
      " 1713.11889648 2001.46972656 2452.32543945 2333.10961914 2712.39208984\n",
      " 2850.05664062 1659.73937988 1636.59875488 1882.75732422 1779.85546875\n",
      " 1165.31652832 1319.59204102 1377.31298828 1043.84765625  470.31530762\n",
      "  635.47808838 1087.21069336 1415.09936523 1743.74572754 2718.12451172\n",
      " 1372.91687012 1364.44482422 1674.4263916  1966.73962402 1717.26293945\n",
      " 1744.67053223 2216.61669922 1840.76159668 1825.40429688 1977.26269531\n",
      " 1302.35070801 1366.93237305 1777.08081055 1317.96044922 1578.09106445\n",
      "  998.64819336  983.61895752 1083.33398438  490.90380859 1541.1862793\n",
      " 1386.70544434]\n",
      "20239.341118280205 mse\n",
      "142.2650382851676 rmse\n",
      "0.09597945008294875 mape\n",
      "0.9241030661312442 r2\n",
      "305784.0 fit time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/nz423/home/miniforge3/envs/hydraboost/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_regressor(\n",
    "    TSMLGridSearchWrapper(\n",
    "        \"holdout\",\n",
    "        seed=0,\n",
    "        modelClass=HydraBoost,\n",
    "        model_param_grid={\n",
    "            \"n_layers\": [2],              # [0,1,3,6,10] ?\n",
    "            \"init_n_kernels\": [8],\n",
    "            \"init_n_groups\": [64],\n",
    "            \"n_kernels\": [8],\n",
    "            \"n_groups\": [64],\n",
    "            \"max_num_channels\": [3],\n",
    "            \"hydra_batch_size\": [10000],\n",
    "            \"l2_reg\": [10],                # [0.01, 0.1, 1, 10] ?\n",
    "            \"l2_ghat\": [0.1],          # [0.01, 0.1, 1, 10] ?\n",
    "            \"boost_lr\": [1],\n",
    "            \"train_top_at\": [[0, 1, 5, 10]],\n",
    "        },\n",
    "    ),\n",
    "regressor_name = \"HydraBoostGridSearch\",\n",
    ") #138.244700694982 rmse   ghat 0.01\n",
    "# 138.11954862031027 rmse  ghat 0.1\n",
    "# 132.26738702131547 rmse  ghat 0.1  l2reg 10\n",
    "# 132.6139792366581 rmse   ghat 0.1  l2reg 10  n_layers 2\n",
    "# 141.6371024323461 rmse   ghat 0.1  l2reg 100\n",
    "\n",
    "#TODO MODIFY ALL VIRTUALENV TO PRINT BEST PARAMS (variable \"second\" in experiments.run_regression_experiment)\n",
    "\n",
    "#also TODO, rerun this and see that it works with second and all....\n",
    "\n",
    "\n",
    "\n",
    "# 132.6139792366581 rmse    WITH RETRAIN TOP AT 1\n",
    "# 0.11765984882729758 mape\n",
    "# 0.9340512642088609 r2\n",
    "# 73529.0 fit time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name\t: Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "!cat /proc/cpuinfo | grep \"model name\" | head -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSMLOptunaWrapper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
