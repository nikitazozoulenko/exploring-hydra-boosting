{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for running test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nz423\\Code\\exploring-hydra-boosting\\.venv\\Lib\\site-packages\\aeon\\base\\__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from load_datasets import get_aeon_dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tsml_eval.experiments import experiments, get_regressor_by_name, run_regression_experiment\n",
    "from tsml_eval.evaluation.storage import load_regressor_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_regressor(\n",
    "        regressor, #= TSMLWrapperHydraBoost(),\n",
    "        regressor_name = \"HydraBoost\",\n",
    "    ):\n",
    "    #get HouseholdPowerConsumption1 dataset\n",
    "    current_dir = Path(os.path.dirname(os.getcwd()))\n",
    "    TSER_data_dir = current_dir.parent / \"Data\" / \"TSER\"\n",
    "    dataset_name = \"HouseholdPowerConsumption1\"\n",
    "    X_train, y_train, X_test, y_test = get_aeon_dataset(dataset_name, TSER_data_dir, \"regression\")\n",
    "\n",
    "    #run regression experiment\n",
    "    run_regression_experiment(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        regressor,\n",
    "        regressor_name=regressor_name,\n",
    "        results_path=\"results/\",\n",
    "        dataset_name=dataset_name,\n",
    "        resample_id=0,\n",
    "    )\n",
    "    rr = load_regressor_results(\n",
    "        current_dir / \"exploring-hydra-boosting\" /\"results\" / regressor_name / \"Predictions\" / dataset_name / \"testResample0.csv\"\n",
    "    )\n",
    "    print(rr.predictions)\n",
    "    print(rr.mean_squared_error, \"mse\")\n",
    "    print(rr.root_mean_squared_error, \"rmse\")\n",
    "    print(rr.mean_absolute_percentage_error, \"mape\")\n",
    "    print(rr.r2_score, \"r2\")\n",
    "    print(rr.fit_time, \"fit time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Wrapper no gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin\n",
    "from tsml.base import BaseTimeSeriesEstimator\n",
    "\n",
    "from models.random_feature_representation_boosting import HydraBoost\n",
    "\n",
    "\n",
    "class TSMLWrapperHydraBoost(RegressorMixin, BaseTimeSeriesEstimator):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TSMLWrapperHydraBoost, self).__init__()\n",
    "        self.hydraboost = HydraBoost(\n",
    "            n_layers=1,\n",
    "            init_n_kernels=8,\n",
    "            init_n_groups=64,\n",
    "            n_kernels=8,\n",
    "            n_groups=64,\n",
    "            max_num_channels=3,\n",
    "            hydra_batch_size=10000,\n",
    "            l2_reg=10,\n",
    "            l2_ghat=0.1,\n",
    "            boost_lr=1,\n",
    "            train_top_at = [0, 5, 10],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> object:\n",
    "        \"\"\"Fit the estimator to training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "        y : 1D np.ndarray of shape (n_instances)\n",
    "            The target labels for fitting, indices correspond to instance indices in X\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        y = y.unsqueeze(1)\n",
    "        self.X_mean = X.mean()\n",
    "        self.X_std = X.std()\n",
    "        self.y_mean = y.mean()\n",
    "        self.y_std = y.std()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        y = (y - self.y_mean) / self.y_std\n",
    "        self.hydraboost.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predicts labels for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape (n_instances)\n",
    "            Predicted target labels.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        pred = self.hydraboost(X)\n",
    "        pred = pred * self.y_std + self.y_mean\n",
    "        return pred.squeeze().detach().numpy()\n",
    "        \n",
    "        \n",
    "\n",
    "    def _more_tags(self) -> dict:\n",
    "        return {\n",
    "            \"X_types\": [\"3darray\"],\n",
    "            \"equal_length_only\": True,\n",
    "            \"allow_nan\": False,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_regressor(\n",
    "#     regressor = TSMLWrapperHydraBoost(),\n",
    "#     regressor_name = \"HydraBoost\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SKLearnWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, modelClass=None, **model_params,):\n",
    "        self.modelClass = modelClass\n",
    "        self.model_params = model_params\n",
    "        self.seed = None\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        self.modelClass = params.pop('modelClass', self.modelClass)\n",
    "        self.seed = params.pop('seed', self.seed)\n",
    "        self.model_params.update(params)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {'modelClass': self.modelClass}\n",
    "        params.update(self.model_params)\n",
    "        return params\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        self.model = self.modelClass(**self.model_params)\n",
    "        self.model.fit(X, y)\n",
    "        # #classes, either label for binary or one-hot for multiclass\n",
    "        # if len(y.size()) == 1 or y.size(1) == 1:\n",
    "        #     self.classes_ = np.unique(y.detach().cpu().numpy())\n",
    "        # else:\n",
    "        #     self.classes_ = np.unique(y.argmax(axis=1).detach().cpu().numpy())\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model(X).squeeze()#.detach().cpu().squeeze().numpy()\n",
    "        # #binary classification\n",
    "        # if len(self.classes_) == 2:\n",
    "        #     proba_1 = torch.sigmoid(self.model(X))\n",
    "        #     return (proba_1 > 0.5).detach().cpu().numpy()\n",
    "        # else:\n",
    "        #     #multiclass\n",
    "        #     return torch.argmax(self.model(X), dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    # def predict_proba(self, X):\n",
    "    #     #binary classification\n",
    "    #     if len(self.classes_) == 2:\n",
    "    #         proba_1 = torch.nn.functional.sigmoid(self.model(X))\n",
    "    #         return torch.cat((1 - proba_1, proba_1), dim=1).detach().cpu().numpy()\n",
    "    #     else:\n",
    "    #         #multiclass\n",
    "    #         logits = self.model(X)\n",
    "    #         proba = torch.nn.functional.softmax(logits, dim=1)\n",
    "    #         return proba.detach().cpu().numpy()\n",
    "    \n",
    "    # def decision_function(self, X):\n",
    "    #     logits = self.model(X)\n",
    "    #     return logits.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "    # def score(self, X, y):\n",
    "    #     logits = self.model(X)\n",
    "    #     if y.size(1) == 1:\n",
    "    #         y_true = y.detach().cpu().numpy()\n",
    "    #         y_score = logits.detach().cpu().numpy()\n",
    "    #         auc = roc_auc_score(y_true, y_score)\n",
    "    #         return auc\n",
    "    #     else:\n",
    "    #         pred = torch.argmax(logits, dim=1)\n",
    "    #         y = torch.argmax(y, dim=1)\n",
    "    #         acc = (pred == y).float().mean()\n",
    "    #         return acc.detach().cpu().item()\n",
    "    \n",
    "    \n",
    "    \n",
    "class TSMLGridSearchWrapper(RegressorMixin, BaseTimeSeriesEstimator):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 holdour_or_kfold: Literal[\"holdout\", \"kfold\"] = \"kfold\",\n",
    "                 kfolds: Optional[int] = 5,\n",
    "                 holdout_percentage: Optional[float] = 0.2,\n",
    "                 seed: Optional[int] = None,\n",
    "                 modelClass=None, \n",
    "                 model_param_grid: Dict[str, List[Any]] = {}\n",
    "        ):\n",
    "        self.holdour_or_kfold = holdour_or_kfold\n",
    "        self.kfolds = kfolds\n",
    "        self.holdout_percentage = holdout_percentage\n",
    "        self.seed = seed\n",
    "        self.modelClass = modelClass\n",
    "        self.model_param_grid = model_param_grid\n",
    "        super(TSMLGridSearchWrapper, self).__init__()\n",
    "        \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> object:\n",
    "        \"\"\"Fit the estimator to training data, with gridsearch hyperparameter optimization\n",
    "        on holdout or kfold cross-validation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "        y : 1D np.ndarray of shape (n_instances)\n",
    "            The target labels for fitting, indices correspond to instance indices in X\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "        \"\"\"\n",
    "        # TODO regression only\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        y = y.unsqueeze(1)\n",
    "        self.X_mean = X.mean()\n",
    "        self.X_std = X.std()\n",
    "        self.y_mean = y.mean()\n",
    "        self.y_std = y.std()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        y = (y - self.y_mean) / self.y_std\n",
    "        \n",
    "        # Configure cross validation\n",
    "        if self.holdour_or_kfold == \"kfold\":\n",
    "            cv = KFold(n_splits=self.kfolds, shuffle=True, random_state=self.seed)\n",
    "        else:  # holdout\n",
    "            cv = ShuffleSplit(n_splits=1, test_size=self.holdout_percentage, random_state=self.seed)\n",
    "                \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=SKLearnWrapper(modelClass=self.modelClass),\n",
    "            param_grid={**self.model_param_grid, \"seed\": [self.seed]},\n",
    "            cv=cv,\n",
    "            scoring=\"neg_mean_squared_error\", # TODO regression only???\n",
    "        )\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # Store best model\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "        print(\"self.best_params\", self.best_params)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predicts labels for sequences in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 3D np.ndarray of shape (n_instances, n_channels, n_timepoints)\n",
    "            The training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape (n_instances)\n",
    "            Predicted target labels.\n",
    "        \"\"\"\n",
    "        X = torch.from_numpy(X).float()\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        pred = self.best_model.predict(X) #TODO regression only?\n",
    "        pred = pred * self.y_std + self.y_mean\n",
    "        return pred.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "\n",
    "    def _more_tags(self) -> dict:\n",
    "        return {\n",
    "            \"X_types\": [\"3darray\"],\n",
    "            \"equal_length_only\": True,\n",
    "            \"allow_nan\": False,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def get_params(self):\n",
    "        \"\"\"Use for saving model configuration in tsml\"\"\"\n",
    "        if hasattr(self, 'best_params'):\n",
    "            return {\n",
    "            \"seed\": self.seed,\n",
    "            **self.best_params\n",
    "            }\n",
    "        else:\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[9.3420e-01, 1.4848e+00, 6.0643e-03,  ..., 1.1000e+01, 0.0000e+00,\n",
      "         9.0000e+00],\n",
      "        [1.7452e+00, 2.7932e+00, 4.5203e-01,  ..., 1.1300e+02, 1.6500e+02,\n",
      "         1.8000e+02],\n",
      "        [1.5180e+00, 2.5001e+00, 3.9788e-01,  ..., 1.2900e+02, 1.6900e+02,\n",
      "         1.5300e+02],\n",
      "        ...,\n",
      "        [1.5690e+00, 2.7859e+00, 4.0276e-01,  ..., 1.1900e+02, 1.9600e+02,\n",
      "         1.8500e+02],\n",
      "        [1.4198e+00, 2.2397e+00, 1.9726e-01,  ..., 1.1800e+02, 2.0000e+02,\n",
      "         1.7900e+02],\n",
      "        [2.4941e+00, 3.6901e+00, 6.8267e-01,  ..., 1.1200e+02, 1.8400e+02,\n",
      "         1.9200e+02]])\n",
      "W Parameter containing:\n",
      "tensor([[ 3.8904e-04,  2.1385e-05, -5.6805e-04,  ..., -9.5189e-05,\n",
      "          6.2680e-04, -9.2276e-04]], requires_grad=True)\n",
      "b tensor([[0.0030]])\n",
      "self.X_mean tensor([[ 4.2483e-07, -2.5642e-07, -5.5004e-08,  ..., -1.7441e-07,\n",
      "          3.2629e-02, -8.0486e-07]])\n",
      "self.y_mean tensor([[-0.0030]])\n",
      "self.y_std tensor([[1.0011]])\n",
      "training W0\n",
      "Phi0 shape torch.Size([596, 8192])\n",
      "X tensor([[7.8606e-03, 1.8674e-03, 7.7422e-03,  ..., 0.0000e+00, 3.0000e+00,\n",
      "         3.0000e+00],\n",
      "        [4.2783e-01, 1.1309e+00, 9.3445e-01,  ..., 5.6000e+01, 1.3100e+02,\n",
      "         1.7300e+02],\n",
      "        [4.7958e-01, 7.7575e-01, 6.8664e-01,  ..., 5.9000e+01, 1.5200e+02,\n",
      "         2.0200e+02],\n",
      "        ...,\n",
      "        [3.4329e-01, 1.1751e+00, 7.6160e-01,  ..., 6.1000e+01, 1.2800e+02,\n",
      "         2.0300e+02],\n",
      "        [2.3119e-01, 6.4996e-01, 4.8736e-01,  ..., 0.0000e+00, 4.4000e+01,\n",
      "         6.6000e+01],\n",
      "        [8.4767e-01, 1.7562e+00, 1.7030e+00,  ..., 3.1000e+01, 1.4500e+02,\n",
      "         2.0300e+02]])\n",
      "W Parameter containing:\n",
      "tensor([[-0.0007, -0.0047, -0.0003,  ..., -0.0056,  0.0071,  0.0048],\n",
      "        [-0.0007, -0.0047, -0.0003,  ..., -0.0056,  0.0071,  0.0048],\n",
      "        [ 0.0007,  0.0047,  0.0003,  ...,  0.0056, -0.0071, -0.0048],\n",
      "        ...,\n",
      "        [ 0.0007,  0.0047,  0.0003,  ...,  0.0056, -0.0071, -0.0048],\n",
      "        [-0.0007, -0.0047, -0.0003,  ..., -0.0056,  0.0071,  0.0048],\n",
      "        [ 0.0007,  0.0047,  0.0003,  ...,  0.0056, -0.0071, -0.0048]],\n",
      "       requires_grad=True)\n",
      "b tensor([[-1.9660e-04, -1.0807e-05,  2.8705e-04,  ...,  4.8101e-05,\n",
      "         -3.1674e-04,  4.6630e-04]])\n",
      "self.X_mean tensor([[1.8001e-08, 2.2802e-08, 1.2801e-07,  ..., 6.8675e-02, 2.4174e-02,\n",
      "         2.7511e-02]])\n",
      "self.y_mean tensor([[-1.4506e-04, -7.9736e-06,  2.1181e-04,  ...,  3.5493e-05,\n",
      "         -2.3371e-04,  3.4407e-04]])\n",
      "self.y_std tensor([[0.0057, 0.0003, 0.0083,  ..., 0.0014, 0.0092, 0.0136]])\n",
      "X tensor([[2.9213e-03, 1.3280e+00, 1.7724e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [7.8389e-01, 4.0263e+00, 4.1384e+00,  ..., 2.5400e+02, 1.8600e+02,\n",
      "         1.2600e+02],\n",
      "        [5.2290e-01, 3.6536e+00, 4.1150e+00,  ..., 1.2900e+02, 1.7900e+02,\n",
      "         1.3000e+02],\n",
      "        ...,\n",
      "        [6.4513e-01, 3.8548e+00, 4.2101e+00,  ..., 1.8200e+02, 1.6100e+02,\n",
      "         1.4500e+02],\n",
      "        [4.2423e-01, 2.9838e+00, 3.1625e+00,  ..., 2.0700e+02, 1.4700e+02,\n",
      "         1.2000e+02],\n",
      "        [6.3747e-01, 4.4474e+00, 4.5349e+00,  ..., 1.5100e+02, 2.0400e+02,\n",
      "         1.1500e+02]])\n",
      "W Parameter containing:\n",
      "tensor([[-0.0101,  0.0086,  0.0015,  ..., -0.0092, -0.0003,  0.0013],\n",
      "        [-0.0101,  0.0086,  0.0015,  ..., -0.0092, -0.0003,  0.0013],\n",
      "        [ 0.0101, -0.0086, -0.0015,  ...,  0.0092,  0.0003, -0.0013],\n",
      "        ...,\n",
      "        [ 0.0101, -0.0086, -0.0015,  ...,  0.0092,  0.0003, -0.0013],\n",
      "        [-0.0101,  0.0086,  0.0015,  ..., -0.0092, -0.0003,  0.0013],\n",
      "        [ 0.0101, -0.0086, -0.0015,  ...,  0.0092,  0.0003, -0.0013]],\n",
      "       requires_grad=True)\n",
      "b tensor([[ 1.6150e-03,  8.8771e-05, -2.3580e-03,  ..., -3.9514e-04,\n",
      "          2.6019e-03, -3.8305e-03]])\n",
      "self.X_mean tensor([[ 3.0682e-07, -6.5705e-07, -5.1324e-07,  ...,  1.1969e-02,\n",
      "          1.0801e-02,  1.2793e-02]])\n",
      "self.y_mean tensor([[ 0.0019,  0.0001, -0.0027,  ..., -0.0005,  0.0030, -0.0045]])\n",
      "self.y_std tensor([[0.0054, 0.0003, 0.0079,  ..., 0.0013, 0.0087, 0.0128]])\n",
      "X tensor([[  1.5803,   2.6562,   0.3682,  ...,  88.0000, 207.0000, 170.0000],\n",
      "        [  2.2211,   3.4802,   0.4905,  ..., 106.0000, 202.0000, 174.0000],\n",
      "        [  2.4442,   3.7649,   1.0696,  ..., 115.0000, 201.0000, 184.0000],\n",
      "        ...,\n",
      "        [  1.3552,   2.1688,   0.2603,  ..., 109.0000, 192.0000, 151.0000],\n",
      "        [  2.4196,   3.5335,   0.6278,  ..., 112.0000, 183.0000, 183.0000],\n",
      "        [  1.5319,   2.4830,   0.3791,  ..., 118.0000, 201.0000, 159.0000]])\n",
      "W Parameter containing:\n",
      "tensor([[ 3.5099e-04,  9.3754e-05, -4.7425e-04,  ..., -3.1010e-05,\n",
      "          2.6375e-04, -1.3394e-03]], requires_grad=True)\n",
      "b tensor([[0.0040]])\n",
      "self.X_mean tensor([[-4.6340e-07,  6.9893e-07,  1.6641e-07,  ...,  2.8418e-07,\n",
      "          2.7596e-02, -1.2571e-06]])\n",
      "self.y_mean tensor([[9.9208e-08]])\n",
      "self.y_std tensor([[1.]])\n",
      "training W0\n",
      "Phi0 shape torch.Size([745, 8192])\n",
      "X tensor([[  0.3985,   1.1394,   0.6818,  ...,  10.0000,  98.0000, 169.0000],\n",
      "        [  0.8175,   1.7537,   1.5736,  ...,  50.0000, 126.0000, 199.0000],\n",
      "        [  1.2547,   1.7168,   2.0541,  ...,  65.0000, 109.0000, 189.0000],\n",
      "        ...,\n",
      "        [  0.4191,   0.6760,   0.5470,  ...,  54.0000, 118.0000, 156.0000],\n",
      "        [  1.1805,   1.4152,   1.7678,  ...,  63.0000, 127.0000, 219.0000],\n",
      "        [  0.4992,   0.8296,   0.7106,  ...,  38.0000,  93.0000, 123.0000]])\n",
      "W Parameter containing:\n",
      "tensor([[-0.0016, -0.0052, -0.0005,  ..., -0.0108,  0.0051,  0.0064],\n",
      "        [-0.0016, -0.0052, -0.0005,  ..., -0.0108,  0.0051,  0.0064],\n",
      "        [ 0.0016,  0.0052,  0.0005,  ...,  0.0108, -0.0051, -0.0064],\n",
      "        ...,\n",
      "        [ 0.0016,  0.0052,  0.0005,  ...,  0.0108, -0.0051, -0.0064],\n",
      "        [-0.0016, -0.0052, -0.0005,  ..., -0.0108,  0.0051,  0.0064],\n",
      "        [ 0.0016,  0.0052,  0.0005,  ...,  0.0108, -0.0051, -0.0064]],\n",
      "       requires_grad=True)\n",
      "b tensor([[-1.0079e-04, -2.6922e-05,  1.3618e-04,  ...,  8.9047e-06,\n",
      "         -7.5736e-05,  3.8461e-04]])\n",
      "self.X_mean tensor([[-4.6404e-07,  4.9284e-08,  2.2402e-07,  ...,  6.1080e-02,\n",
      "          2.9106e-02,  3.2935e-02]])\n",
      "self.y_mean tensor([[-8.3013e-05, -2.2174e-05,  1.1217e-04,  ...,  7.3343e-06,\n",
      "         -6.2381e-05,  3.1678e-04]])\n",
      "self.y_std tensor([[0.0051, 0.0014, 0.0069,  ..., 0.0005, 0.0039, 0.0196]])\n",
      "X tensor([[  0.4630,   3.2540,   3.4863,  ..., 117.0000, 153.0000, 125.0000],\n",
      "        [  0.5906,   4.0635,   4.1917,  ..., 182.0000, 208.0000, 119.0000],\n",
      "        [  0.4260,   3.4302,   3.7733,  ..., 104.0000, 202.0000, 117.0000],\n",
      "        ...,\n",
      "        [  0.6877,   3.7969,   2.9875,  ..., 173.0000, 280.0000,  81.0000],\n",
      "        [  0.5039,   3.9016,   4.2631,  ..., 142.0000, 152.0000, 117.0000],\n",
      "        [  0.9003,   4.6230,   3.9056,  ..., 188.0000, 269.0000, 110.0000]])\n",
      "W Parameter containing:\n",
      "tensor([[-0.0119,  0.0105, -0.0007,  ..., -0.0025, -0.0004,  0.0014],\n",
      "        [-0.0119,  0.0105, -0.0007,  ..., -0.0025, -0.0004,  0.0014],\n",
      "        [ 0.0119, -0.0105,  0.0007,  ...,  0.0025,  0.0004, -0.0014],\n",
      "        ...,\n",
      "        [ 0.0119, -0.0105,  0.0007,  ...,  0.0025,  0.0004, -0.0014],\n",
      "        [-0.0119,  0.0105, -0.0007,  ..., -0.0025, -0.0004,  0.0014],\n",
      "        [ 0.0119, -0.0105,  0.0007,  ...,  0.0025,  0.0004, -0.0014]],\n",
      "       requires_grad=True)\n",
      "b tensor([[ 4.0772e-04,  1.0891e-04, -5.5090e-04,  ..., -3.6022e-05,\n",
      "          3.0638e-04, -1.5559e-03]])\n",
      "self.X_mean tensor([[ 2.8098e-07, -1.3377e-07, -5.2900e-07,  ...,  9.7229e-03,\n",
      "          8.6940e-03,  1.0328e-02]])\n",
      "self.y_mean tensor([[ 5.9577e-04,  1.5914e-04, -8.0500e-04,  ..., -5.2637e-05,\n",
      "          4.4770e-04, -2.2735e-03]])\n",
      "self.y_std tensor([[0.0051, 0.0014, 0.0069,  ..., 0.0005, 0.0038, 0.0194]])\n",
      "self.best_params {'boost_lr': 1, 'hydra_batch_size': 10000, 'init_n_groups': 64, 'init_n_kernels': 8, 'l2_ghat': 0.1, 'l2_reg': 10, 'max_num_channels': 3, 'n_groups': 64, 'n_kernels': 8, 'n_layers': 2, 'seed': 0, 'train_top_at': [0, 5, 10]}\n",
      "[ 1667.18200684  1338.55603027  1204.50170898  1689.02514648\n",
      "  1798.07702637  1437.22949219  1719.75097656  1806.44104004\n",
      "  1644.92285156  2609.0612793   1269.26745605  1974.78063965\n",
      "  1749.21081543  1586.93652344  1358.04248047  1599.18920898\n",
      "  1215.68261719  1295.69311523  1188.9465332    733.34094238\n",
      "   593.93322754   976.40246582  1545.94213867  2102.56298828\n",
      "  2383.37353516  1873.73168945  1718.95202637  1738.06237793\n",
      "  1993.22583008  2024.484375    2099.05615234  1945.15454102\n",
      "  1857.29882812  1828.81030273  1883.18408203  1278.24072266\n",
      "  2135.83129883  1610.73144531  1641.29553223  1166.05700684\n",
      "  1074.8873291   1055.89074707  1050.82067871   628.05749512\n",
      "  1129.60327148  1256.83666992  2114.23925781  1874.37866211\n",
      "  2022.93432617  1675.07971191  2058.55029297  1719.74658203\n",
      "  1726.35498047  2674.57714844  1293.62585449  2554.22631836\n",
      "  1968.83398438  2466.09667969  1634.89355469  1478.01806641\n",
      "  1563.0592041   1248.30651855  1533.89428711  1088.98425293\n",
      "  1161.35961914  1096.56896973   566.09741211  1201.99780273\n",
      "  1420.32897949  1612.68566895  2376.60302734  1179.9317627\n",
      "  1513.78881836  1541.25183105  1649.83947754  1676.57263184\n",
      "  2245.99951172  1980.61865234  1844.9239502   1742.66845703\n",
      "  1773.59082031  1372.97558594  1496.24755859  2050.09594727\n",
      "  1038.23754883  1918.13452148   608.44720459  1056.16967773\n",
      "   921.76776123   538.96984863  2182.97583008  1734.82470703\n",
      "  2134.04785156  1422.84204102  1808.4440918   1696.62756348\n",
      "  1690.31958008  2214.52734375  1776.24743652  2375.21582031\n",
      "  1615.61999512  2560.77001953  1665.6015625   1590.26733398\n",
      "  1302.08642578  1726.32763672 -1202.87255859  1646.08496094\n",
      "   560.99475098  1345.94616699  1271.76623535   566.46374512\n",
      "  1575.66271973  1093.4987793   1605.77270508   789.46734619\n",
      "  1738.83691406  2224.71240234  2653.18261719  1804.52832031\n",
      "  1897.02478027  2573.67504883  1956.37719727  2195.38745117\n",
      "  1575.25012207  1476.4888916   1687.90905762  1501.72094727\n",
      "  1787.95202637  1341.90722656  1012.95904541  1205.02478027\n",
      "   821.60327148   515.29040527  1469.25732422  1409.54016113\n",
      "  1503.48937988  2274.41943359  1433.08447266  1454.10803223\n",
      "  1799.40405273  1897.26220703  2194.01757812  1700.15661621\n",
      "  2114.81542969  1881.86181641  1733.61181641  1099.82958984\n",
      "  1852.47058105  1358.39086914  1999.01245117  1336.34924316\n",
      "  1108.93395996   661.58355713  1531.83203125  1464.39025879\n",
      "  1144.65344238  1424.85986328  1087.01721191  2844.34521484\n",
      "  2153.04003906  1494.87731934  1075.22167969  1610.37023926\n",
      "  1431.98217773  1812.85253906  1407.93115234  1829.66540527\n",
      "  1406.50891113  1284.27392578  1432.9239502   1671.83251953\n",
      "  1808.67480469  2112.69091797  1648.39257812  1792.18005371\n",
      "   502.78662109   847.95788574  1264.08520508  1170.54577637\n",
      "  1649.48364258  1379.81713867  2963.87109375  2039.68139648\n",
      "  2687.46728516   994.42901611  1571.33251953  1580.05078125\n",
      "  1864.61657715   953.53869629  2005.38183594  1913.56677246\n",
      "  1922.42211914  1829.52929688  1649.61767578  1425.86328125\n",
      "  1642.63903809  1045.10986328  1057.65722656   761.31390381\n",
      "   860.01379395   877.95159912  1060.05053711  1295.9876709\n",
      "  1672.32556152  2800.06494141  1965.00061035  1470.7019043\n",
      "  2567.04785156  2589.42138672  1819.07287598  1846.25183105\n",
      "   885.9642334   1847.99609375  1667.28063965  1465.5222168\n",
      "  1555.19812012  1455.86816406  1270.45629883  1453.59887695\n",
      "  1097.12597656  1238.99023438   892.66723633   754.70422363\n",
      "  1165.50183105  1294.50634766  1445.35534668  1701.20410156\n",
      "  2518.07348633  1395.15136719  1761.71313477  1416.33532715\n",
      "  1760.00585938  2252.83032227   796.74963379  1943.09289551\n",
      "  1390.39880371  1447.3536377   1340.4519043   1736.67102051\n",
      "  1563.45837402  1475.83752441  1078.4888916   1468.24145508\n",
      "  1043.3359375    690.93334961   957.92144775  2071.04907227\n",
      "   989.34094238  2260.37768555  1428.74145508  1246.88720703\n",
      "  1838.57788086  1479.69140625  1370.50708008  2198.13696289\n",
      "  1751.25512695  1955.37524414  1514.24536133   875.38604736\n",
      "  1695.56286621  1701.44714355  1386.21582031  2119.12548828\n",
      "  1094.45043945   982.27502441  1190.59338379  1256.75537109\n",
      "   579.33764648   678.22900391  1285.77587891  1347.26904297\n",
      "  1957.81896973  2351.99121094  1648.32080078  1600.0760498\n",
      "  1842.07250977  2264.83837891  1210.90014648  1730.7755127\n",
      "  1908.70458984  1419.21240234   170.08227539  1516.56311035\n",
      "  1799.59692383  1402.47485352  1130.51416016  1187.04614258\n",
      "  1586.61999512   691.14453125   821.21246338   992.82434082\n",
      "  2037.74389648  1757.11730957  1889.10144043  1893.42944336\n",
      "  1881.94335938  1407.32958984  2315.15625     1212.54699707\n",
      "  2019.73144531  1328.93457031  2755.59326172  1918.12609863\n",
      "  1012.76550293  1641.91259766  1552.37365723  1436.61279297\n",
      "  1326.04431152   861.88964844  1819.06262207   816.09570312\n",
      "  1045.30126953  1373.09545898  1334.37329102  1036.56152344\n",
      "  1459.73376465  1653.609375    1494.59814453  1696.42700195\n",
      "  2300.28051758  2165.68115234  1984.69970703  2110.9375\n",
      "  1393.09533691  1663.98364258  1688.8918457   1450.14611816\n",
      "  1464.09887695  1094.42553711  2177.54931641  1171.99499512\n",
      "  1641.05627441   729.73754883   889.56182861  1578.51147461\n",
      "   172.33862305  1233.52294922  1535.32299805  1925.29394531\n",
      "  2394.28417969  1749.93432617  2360.04614258  1230.38439941\n",
      "  1604.03869629  1674.58056641  2525.80932617  1480.83410645\n",
      "  1403.16027832  1825.11584473  1126.44250488  1224.3371582\n",
      "  1519.48706055  1383.60827637  1152.3425293   1359.75024414\n",
      "  1112.8885498    960.48681641  1156.29101562   843.52752686\n",
      "  1610.05737305  1475.55712891  2195.56152344  2049.89257812\n",
      "  2712.97070312  2313.05175781  1893.85083008  1880.40307617\n",
      "  2828.63037109  1754.95996094  1873.45935059  1580.12487793\n",
      "  1833.92150879  1136.51086426  1602.30688477  1384.22106934\n",
      "  1375.98620605  1589.03491211  1299.17053223   539.36877441\n",
      "   568.86743164  1359.47814941   738.74890137  1537.515625\n",
      "  1317.04553223  2613.51489258  1871.44384766  2355.66357422\n",
      "  1055.35021973  1942.01049805  1252.76098633  2108.23217773\n",
      "  1192.76477051  1700.72644043  2043.65356445  1574.76586914\n",
      "  2011.1229248   1407.015625    1460.43225098  1265.06542969\n",
      "   842.3392334   1100.16394043   411.31555176   865.84490967\n",
      "   975.41870117  1016.30657959  1265.63317871  -338.89746094\n",
      "  1575.15344238  1874.58984375  1119.66333008  1320.14221191\n",
      "  1628.80212402  2235.15722656  1204.14074707  2127.44091797\n",
      "  1217.81201172  1502.72009277  1829.68481445   970.26953125\n",
      "  1563.08459473  1295.67016602  1431.01428223  1084.97937012\n",
      "   482.41638184   844.14263916  1172.64660645   852.87268066\n",
      "  1565.01965332  1446.81555176  2459.02001953  1247.06103516\n",
      "  1369.10412598  1445.44494629  1996.96740723  2190.53564453\n",
      "  1864.70983887  1660.89685059  1692.03466797  1296.87976074\n",
      "  1147.25610352  1586.09643555  1373.75805664  1789.98376465\n",
      "  1389.11669922   651.06713867  1096.05395508   711.45581055\n",
      "  1968.42565918  1467.9140625   2507.63623047  1790.46032715\n",
      "  1182.62109375  1534.80126953  1819.95861816  2174.12402344\n",
      "  1877.20837402   688.06195068  2205.13769531  1926.07885742\n",
      "  1822.09777832   847.82202148  1345.7677002   1496.38598633\n",
      "  1017.51763916  1480.50427246  1063.16552734   921.07739258\n",
      "  1198.28808594  1378.66418457  1292.20214844  1484.67614746\n",
      "  1581.25671387  1967.88391113  1781.6887207   1281.06738281\n",
      "  1803.93847656  1940.32897949  2337.70361328  1520.57507324\n",
      "  1720.14367676  1139.31811523  1252.88842773  2054.62207031\n",
      "  1106.93688965  1386.55249023  1061.80834961   572.21380615\n",
      "  1304.44287109  1496.0177002   1237.81115723  1645.97558594\n",
      "  1946.89575195  2245.85620117  1682.21923828  1937.99645996\n",
      "  1966.8449707   1311.28417969  1594.54394531  1910.34667969\n",
      "  2636.11865234  1823.95019531  1023.60479736  1375.06213379\n",
      "  1828.53210449  1668.44604492  1578.72717285  1274.34436035\n",
      "  1484.51159668   938.09094238  1266.2130127    506.68591309\n",
      "   443.32324219  1541.60168457  1357.81335449  1975.97998047\n",
      "  2681.87939453  1633.11303711  2019.66162109  1760.5625\n",
      "  2597.79980469  1589.57116699  1753.50354004  1763.62329102\n",
      "  1072.70947266  1640.72167969  1657.88708496  1235.11413574\n",
      "  1105.92370605   738.65570068   607.9510498   1505.68041992\n",
      "  1466.99304199  1790.59350586  1352.44104004  3231.8972168\n",
      "  1660.09729004  2034.64025879  1310.23657227  2188.109375\n",
      "  1959.1550293   1930.65209961  1124.59423828  1461.54638672\n",
      "  1034.57275391   602.04858398  1430.78979492  1342.94714355\n",
      "  1692.33740234  1874.36376953  1549.53979492  1622.04699707\n",
      "  1782.64440918  2516.46411133  1770.28515625  2698.56030273\n",
      "  2214.87036133  2042.07470703   547.22802734  2324.11303711\n",
      "  1613.69396973  2022.10388184  1313.05761719   941.50805664\n",
      "  1648.29528809  1476.69494629  1374.90979004   469.9588623\n",
      "   535.29370117  1090.61706543  1507.77905273  1558.95129395\n",
      "  2260.23535156  1353.65600586  1676.75756836  1629.56384277\n",
      "  1705.51904297  2193.87109375  1804.26489258  2012.65161133\n",
      "  1462.33203125  1280.97558594  1580.98388672  1275.61376953\n",
      "  1692.51159668  1754.58300781  1271.8059082   2283.6730957\n",
      "  1353.9967041   1089.06164551   648.98638916   638.44622803\n",
      "  1575.01123047  1317.33984375  1773.2590332   2006.91918945\n",
      "  1430.14855957  1361.25024414  1496.82556152  1692.97839355\n",
      "  2301.09228516  1936.72119141  2505.54248047  1912.06762695\n",
      "   707.76342773  1581.57165527  1765.07275391  1521.36450195\n",
      "  1258.06323242  1460.24316406  1909.87194824  1052.74926758\n",
      "  1041.60913086   710.7623291    577.33654785  1190.11364746\n",
      "  1615.24060059  2021.45898438  1932.27111816  1691.13647461\n",
      "  1832.23779297  1541.75817871  1628.71044922  1789.94433594\n",
      "  3308.83691406  1742.49731445  1721.04956055  1443.88891602\n",
      "  1591.00805664  1683.23864746   933.76586914  1753.28979492\n",
      "  1229.00280762  1079.48852539  1311.45727539   945.89666748\n",
      "   513.41320801   656.32873535  1252.48254395  1552.67993164\n",
      "  1555.0546875   1776.62414551  1495.57983398  1485.65893555\n",
      "  2419.34008789  1738.3170166   2021.85461426  2448.34887695\n",
      "  2352.32177734  2714.95800781  2902.59204102  1673.35327148\n",
      "  1611.69226074  1873.796875    1754.28222656  1182.1829834\n",
      "  1339.76843262  1380.8248291   1044.15820312   479.97595215\n",
      "   658.1739502   1094.09814453  1445.79370117  1738.40039062\n",
      "  2778.28759766  1356.92578125  1388.38146973  1687.71838379\n",
      "  1973.23815918  1716.14868164  1797.63317871  2243.84960938\n",
      "  1843.19287109  1873.85437012  1955.79821777  1293.82067871\n",
      "  1353.25463867  1784.61633301  1315.37976074  1597.90441895\n",
      "  1019.52252197   993.65869141  1075.19360352   499.5369873\n",
      "  1555.17248535  1377.94543457]\n",
      "21953.380888055843 mse\n",
      "148.16673340549775 rmse\n",
      "0.20652448181482536 mape\n",
      "0.9176754674117591 r2\n",
      "117281.0 fit time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nz423\\Code\\exploring-hydra-boosting\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_regressor(\n",
    "    TSMLGridSearchWrapper(\n",
    "        \"holdout\",\n",
    "        seed=0,\n",
    "        modelClass=HydraBoost,\n",
    "        model_param_grid={\n",
    "            \"n_layers\": [1],              # [0,1,3,6,10] ?\n",
    "            \"init_n_kernels\": [8],\n",
    "            \"init_n_groups\": [64],\n",
    "            \"n_kernels\": [8],\n",
    "            \"n_groups\": [64],\n",
    "            \"max_num_channels\": [3],\n",
    "            \"hydra_batch_size\": [10000],\n",
    "            \"l2_reg\": [10],                # [0.01, 0.1, 1, 10] ?\n",
    "            \"l2_ghat\": [0.1],          # [0.01, 0.1, 1, 10] ?\n",
    "            \"boost_lr\": [1],\n",
    "            \"train_top_at\": [[0,1,  5, 10]],\n",
    "        },\n",
    "    ),\n",
    "regressor_name = \"HydraBoostGridSearch\",\n",
    ") #138.244700694982 rmse   ghat 0.01\n",
    "# 138.11954862031027 rmse  ghat 0.1\n",
    "# 132.26738702131547 rmse  ghat 0.1  l2reg 10\n",
    "# 132.6139792366581 rmse   ghat 0.1  l2reg 10  n_layers 2\n",
    "# 141.6371024323461 rmse   ghat 0.1  l2reg 100\n",
    "\n",
    "#TODO MODIFY ALL VIRTUALENV TO PRINT BEST PARAMS (variable \"second\" in experiments.run_regression_experiment)\n",
    "\n",
    "#also TODO, rerun this and see that it works with second and all....\n",
    "\n",
    "\n",
    "\n",
    "# 132.6139792366581 rmse    WITH RETRAIN TOP AT 1\n",
    "# 0.11765984882729758 mape\n",
    "# 0.9340512642088609 r2\n",
    "# 73529.0 fit time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSMLOptunaWrapper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
